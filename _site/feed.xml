<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Laksh Gupta</title>
    <description>A Blog for Some Stuff</description>
    <link>http://lakshgupta.github.io/</link>
    <atom:link href="http://lakshgupta.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>2015-06-14 02:46:23 -0700</pubDate>
    <lastBuildDate>2015-06-14 02:46:23 -0700</lastBuildDate>
    <generator>Jekyll v</generator>
    
      <item>
        <title>Word Vectors</title>
        <description>
</description>
        <pubDate>2015-06-12 05:00:00 -0700</pubDate>
        <link>http://lakshgupta.github.io/2015/06/12/Word+Vectors/</link>
        <guid isPermaLink="true">http://lakshgupta.github.io/2015/06/12/Word+Vectors/</guid>
        
        
      </item>
    
      <item>
        <title>Linear Regression</title>
        <description>&lt;p&gt;Alright, in the last &lt;a href=&quot;http://lakshgupta.github.io/2015/05/21/ArtificialNeuron/&quot;&gt;post&lt;/a&gt; we looked at the very basic building block of a neural network: a neuron. But what could possibly a single neuron be good for? Well, as I mentioned in my last post it can be used to learn very simple models. Let us try to solve a linear regression problem using a neuron. &lt;/p&gt;

&lt;blockquote&gt;
Linear regression is the simplest form of regression.  We model our system with a linear combination of features to produce one output.
&lt;p align=&quot;right&quot;&gt;- &lt;a href=&quot;http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1&quot;&gt;Brian Dolhansky&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;I’ll use the problem used in the Andrew Ng’s machine learning course. The dataset is located &lt;a href=&quot;https://github.com/lakshgupta/lakshgupta.github.io/blob/master/data/ex1data1.txt&quot;&gt;here&lt;/a&gt;. We will try to predict the profit for the franchise based on the population of the city. We’ll use the previous data to prepare a model. So let us first understand the data.&lt;/p&gt;

&lt;center&gt;&lt;canvas id=&quot;inputData&quot; width=&quot;600&quot; height=&quot;400&quot;&gt;&lt;/canvas&gt;&lt;/center&gt;

&lt;p&gt;Looking at the data we can say that we don’t need a complex model and linear regression is good enough for our purpose.&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Training a model&lt;/h2&gt;

&lt;center&gt;&lt;canvas id=&quot;artificialneuron&quot; width=&quot;500&quot; height=&quot;150&quot;&gt;&lt;/canvas&gt;&lt;/center&gt;

&lt;p&gt;Our neuron will receive two values as an input. One of them is the actual value from the data and the other is a bias value. We usually include the bias value along with the input feature matrix x.&lt;/p&gt;

&lt;blockquote&gt;
b is the bias, a term that shifts the decision boundary away from the origin and does not depend on any input value.
&lt;p align=&quot;right&quot;&gt;- &lt;a href=&quot;http://en.wikipedia.org/wiki/Perceptron&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since we want to linearly fit the data, we’ll use the linear activation function. When our neuron will receive the inputs, we’ll calculate the weighted sum and consider that as our output from the neuron.&lt;/p&gt;
&lt;center&gt;$$f(x_i,w) = \phi(\sum\limits_{j=0}^n(w^j x_i^j)) = \sum\limits_{j=0}^n(w^j x_i^j) = w^Tx_i$$&lt;/center&gt;
&lt;p&gt;where &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; represents a row of a matrix&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; represetns an element of a matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The other way to look at our setup is that we are trying to fit a line to the data represented as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i = w^0x_i^0 + w^1b&lt;/script&gt;

&lt;p&gt;We then try to figure out how close our neuron output or prediction is from the actual answer, i.e. we’ll apply a &lt;a href=&quot;http://en.wikipedia.org/wiki/Loss_function&quot;&gt;loss function&lt;/a&gt;, also known as a cost function over our dataset. A commonly used one is the least square error:&lt;/p&gt;
&lt;center&gt;$$J(w) = \sum\limits_{i=0}^n(f(x_i,w) - y_i)^2$$&lt;/center&gt;
&lt;p&gt;The idea is to use this value to modify our randomly initialized weight matrix till the time we stop observing the decrease in the cost function value. The method we’ll use to modify the weight matrix is known as &lt;a href=&quot;http://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient Descent&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;$$w = w - \frac{\alpha}{m}\Delta J(w)$$&lt;/center&gt;
&lt;p&gt;here &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is the weight matrix&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the learning rate&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is the size of our data acting as a normalizing factor&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Delta J(w)&lt;/script&gt; is the gradient of the cost function with respect to each of the weight under consideration say weight for the connection between a neuron &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; and a neuron &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial w_{jk}} J(w) = \sum\limits_{i=0}^n 2\left(f(x_i, w)-y_i\right) \frac{\partial}{\partial w_{jk}} f(x_i, w) &lt;/script&gt;

&lt;p&gt;After the gradient descent step, to make a prediction we just need to use the modified weight matrix and apply the same function we used above:&lt;/p&gt;
&lt;center&gt;$$f(x_i,w) = w^Tx_i$$&lt;/center&gt;

&lt;p&gt;So let us train the model and see how it is behaving by plotting the results of the above equation in red using the weight matrix and the x-axis.&lt;/p&gt;
&lt;center&gt;
&lt;canvas id=&quot;fitData&quot; height=&quot;400px&quot; width=&quot;600&quot;&gt;
This text is displayed if your browser does not support HTML5 Canvas.
&lt;/canvas&gt;
&lt;/center&gt;

&lt;!-- ############# JAVASCRIPT ############--&gt;
&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;http://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.4.4/p5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/plot/scatter.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/utils/mathUtils.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/canvas.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/neuron.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/neuralnet.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot;&gt; 
    
  //artificial neuron: linear model
  var _ancanvas = document.getElementById(&quot;artificialneuron&quot;);
  var _anctx = _ancanvas.getContext(&quot;2d&quot;);
  var neuronIn1 = new neuron(_anctx, 50, 40, neuronRadius,&quot;b&quot;);
  var neuronIn2 = new neuron(_anctx, 50, 110, neuronRadius, &quot;x_i^j&quot;);
  var	hiddenLayer= new neuron(_anctx, 200, 75, neuronRadius);
  _anctx.mathText(&quot;f(x_i, w)&quot;,200,120,{&quot;text-align&quot;: &quot;center&quot;});
  var neuronOut = new neuron(_anctx, 350, 75, neuronRadius,&quot;y&quot;);
  //input to hidden layer
  connectLayers([neuronIn1, neuronIn2], [hiddenLayer]);
  //hidden to output layer
  connectLayers([hiddenLayer], [neuronOut]);
  
  var iterations = 500;//1500;
  var learningRate = 0.01;
  
  function setup(){
    loadTable(&quot;/data/ex1data1.txt&quot;,&quot;CSV&quot;,linReg);
  }
  
  function linReg(table){
    var rowCount = table.rows.length - 1;
    var X = Array.matrix(rowCount, 2, 0);
    var Y = Array.matrix(rowCount, 1, 0);
    var J_history = Array.matrix(iterations,1, 0);
    var m = X.length;
    //var theta = numeric.random([2,1]);
    var theta = Array.matrix(2,1,0);
    var xMax = table.getNum(0,0);
    var xMin = table.getNum(0,0);
    var yMax = table.getNum(0,1);
    var yMin = table.getNum(0,1);
    //load X and Y from table
    for(var i=0; i&lt;rowCount; i++){
      X[i][0] = table.getNum(i,0);
      X[i][1] = 1;
      Y[i][0] = table.getNum(i,1);
      //find min and max
      if(xMax &lt; X[i][0]){
        xMax = X[i][0];
      }
      if(xMin &gt; X[i][0]){
        xMin = X[i][0];
      }
      if(yMax &lt; Y[i][0]){
        yMax = Y[i][0];
      }
      if(yMin &gt; Y[i][0]){
        yMin = Y[i][0];
      }
    }
    //plot input data
    var chartInfo= { y:{min:yMin, max:yMax, steps:5,label:&quot;Profit in $10,000s&quot;},
                      x:{min:xMin, max:xMax, steps:5,label:&quot;Population of City in 10,000s&quot;}
    };
    var inputPlot = new scatter(&quot;inputData&quot;,chartInfo, X, Y);
        
    //compute initial cost
    console.log(&quot;Initial cost: &quot;+ computeCost(X,Y, theta));
    
    //run gradient descent
    var itrArray = Array.matrix(iterations,1,0);
    for(var i=0;i&lt;iterations;i++){
	    var tempTheta = theta;
	    //for each weight
	    var subCorrection1 = numeric.sub(numeric.dot(X, tempTheta), Y);
	    for (var j=0; j &lt; theta.length; j++){
	      //for each input row
	      var subCorrection = subCorrection1;
	      for(var k=0;k&lt;m;k++){
		      subCorrection[k] = subCorrection[k]*X[k][j];
	      }
	      //console.log(subCorrection);
	      var correction = (learningRate/m) * numeric.sum(subCorrection);
	      subCorrection = [];
		    //console.log(correction);
		    theta[j][0] = theta[j][0] - correction;
      }
      //Save the cost J in every iteration    
      J_history[i] = computeCost(X, y, theta);
      itrArray[i] = i;
    }
    //console.log(J_history);
    
    //plot the linear fit
    var fitChartInfo= { y:{min:yMin, max:yMax, steps:5,label:&quot;Profit in $10,000s&quot;},
                      x:{min:xMin, max:xMax, steps:5,label:&quot;Population of City in 10,000s&quot;}
    };
    var fitPlot = new scatter(&quot;fitData&quot;,fitChartInfo, X, Y);
    fitPlot.plotLine(theta);
    
  }
  
  
  //loss function
  function computeCost(x,y, theta){
    var m = 1;
    if(Array.isArray(x)){
      m = x.length;
    } 
    return numeric.sum(numeric.pow(numeric.sub(numeric.dot(x, theta), y),2));///(2*m);
  };
&lt;/script&gt;

</description>
        <pubDate>2015-05-27 05:00:00 -0700</pubDate>
        <link>http://lakshgupta.github.io/2015/05/27/LinearRegression/</link>
        <guid isPermaLink="true">http://lakshgupta.github.io/2015/05/27/LinearRegression/</guid>
        
        
      </item>
    
      <item>
        <title>Artificial Neuron</title>
        <description>&lt;p&gt;Billions of neuron work together in a highly parallel manner to form the most sophisticated computing device known as the human brain. A single neuron:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;receives the electrical signals from the axons of other neurons through dendrites. Signals can come from different organs such as eyes and ears.&lt;/li&gt;
  &lt;li&gt;modulates the signals in various amounts at the synapses between the dendrite and axons.&lt;/li&gt;
  &lt;li&gt;fires an output signal only when the total strength of the input signals exceed a certain threshold. This signal is sent further to other neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/img/nn/bioneuron.jpg&quot; alt=&quot;Biological Neuron&quot; /&gt;
&lt;span class=&quot;caption text-muted&quot;&gt;A Biological Neuron&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;More information about a biological neuron can be found on &lt;a href=&quot;http://en.wikipedia.org/wiki/Neuron&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Artificial Neuron&lt;/h2&gt;
&lt;p&gt;An artificial neuron is a mathematical model of a biological neuron. The steps mentined for a biological neuron can be mapped to an artificial neuron as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;an artificial neuron receives the input as numerical values rather than the electrical signals. Input can come from different sources such as an image or a text.&lt;/li&gt;
  &lt;li&gt;it then multiplies each of the input value by a value called the weight.&lt;/li&gt;
  &lt;li&gt;weighted sum is calculated then to represent the total strength of the input signal, and an activation function is applied on the sum to get the output. This output can be sent further to other artificial neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
The artificial neuron receives one or more inputs (representing dendrites) and sums them to produce an output (representing a neuron's axon). Usually the sums of each node are weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded.
&lt;p align=&quot;right&quot;&gt;- &lt;a href=&quot;http://en.wikipedia.org/wiki/Artificial_neuron&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Considering only a single input vector x:&lt;/p&gt;
&lt;center&gt;&lt;canvas id=&quot;artificialneuron&quot; width=&quot;500&quot; heigth=&quot;400&quot;&gt;&lt;/canvas&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f(x,w) = \phi(\sum\limits_{i=0}^n(w_i x_i)) = \phi(w^Tx)&lt;/script&gt;  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; is our activation function.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; are the elements of the input vector x.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; are the elements of the weight vector w. &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is the output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice that in terms of “learning” in almost all of the machine learning algorithms, we learn the weight parameters &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;. &lt;/p&gt;

&lt;h2 class=&quot;section-heading&quot;&gt;Activation Function&lt;/h2&gt;
&lt;p&gt;An artificial neuron using a step activation function is known as a Perceptron. Perceptron can act as a binary classifier based on if the value of the activation function is above or below a threashold. But step activation function may not be a good choice every time.&lt;/p&gt;

&lt;blockquote&gt;
  In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0 to 1. That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way. So while your &quot;9&quot; might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-to-control way. That makes it difficult to see how to gradually modify the weights and biases so that the network gets closer to the desired behaviour.
  &lt;p align=&quot;right&quot;&gt;- &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;Michael Nielsen&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are other activation functions which seem to work generally better in most of the cases, such as tanh or maxout functions.&lt;/p&gt;

&lt;blockquote&gt;
  &quot;What neuron type should I use?&quot; Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of &quot;dead&quot; units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.
  &lt;p align=&quot;right&quot;&gt;- &lt;a href=&quot;http://cs231n.github.io/neural-networks-1/&quot;&gt;Andrej Karpathy&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;
 &lt;canvas id=&quot;step&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&lt;/canvas&gt;
 &lt;canvas id=&quot;sigmoid&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&lt;/canvas&gt;
 &lt;canvas id=&quot;tanh&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&lt;/canvas&gt;
&lt;/center&gt;

&lt;p&gt;This ends the brief overview of a neuron. In the coming posts I’ll try to cover some of the things we can do with a neuron. Till then, enjoy!&lt;/p&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/canvas.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/neuron.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/nn/neuralnet.js&quot;&gt;&lt;/script&gt;

&lt;script language=&quot;javascript&quot; type=&quot;text/javascript&quot; src=&quot;/js/plot/eqgraph.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script&gt;
//artificial neuron
var _ancanvas = document.getElementById(&quot;artificialneuron&quot;);
var _anctx = _ancanvas.getContext(&quot;2d&quot;);
var neuronIn1 = new neuron(_anctx, 50, 40, neuronRadius,&quot;x_0&quot;);
var neuronIn2 = new neuron(_anctx, 50, 110, neuronRadius, &quot;x_n&quot;);
var	hiddenLayer= new neuron(_anctx, 200, 75, neuronRadius);
_anctx.mathText(&quot;f(x,w)&quot;,200,120,{&quot;text-align&quot;: &quot;center&quot;});
var neuronOut = new neuron(_anctx, 350, 75, neuronRadius,&quot;y&quot;);
//input to hidden layer
connectLayers([neuronIn1, neuronIn2], [hiddenLayer]);
//hidden to output layer
connectLayers([hiddenLayer], [neuronOut]);

//plot step
function step(z){ 
        if(z &lt; 2){
          return 0;
        }else{
          return 1;
        }
      }
var stepGraph = new EqGraph({canvasId: 'step', minX: -4, minY: -2, maxX: 4, maxY: 2, unitsPerTick: 1 });
stepGraph.drawEquation(step , 'blue', 2);
var stepCanv = document.getElementById('step');
var stepcontext = stepCanv.getContext('2d');
stepcontext.font = 'italic 14pt Calibri';
stepcontext.fillStyle = '#777';
stepcontext.fillText('step', 10, stepCanv.height-5);

//plot sigmoid
function sigmoid(z){ return  1.0/(1.0+Math.exp(-z));}
var sigmoidGraph = new EqGraph({canvasId: 'sigmoid', minX: -6, minY: -2, maxX: 6, maxY: 2, unitsPerTick: 1 });
sigmoidGraph.drawEquation(sigmoid , 'blue', 2);
var sigmoidCanv = document.getElementById('sigmoid');
var sigmoidcontext = sigmoidCanv.getContext('2d');
sigmoidcontext.font = 'italic 14pt Calibri';
sigmoidcontext.fillStyle = '#777';
sigmoidcontext.fillText('sigmoid', 10, sigmoidCanv.height-5);

//plot tanh
function tanh(z){ return (Math.exp(z)-Math.exp(-z))/(Math.exp(z)+Math.exp(-z));}
var tanhGraph = new EqGraph({canvasId: 'tanh', minX: -6, minY: -2, maxX: 6, maxY: 2, unitsPerTick: 1 });
tanhGraph.drawEquation(tanh , 'blue', 2);
var tanhCanv = document.getElementById('tanh');
var tanhcontext = tanhCanv.getContext('2d');
tanhcontext.font = 'italic 14pt Calibri';
tanhcontext.fillStyle = '#777';
tanhcontext.fillText('tanh', 10, tanhCanv.height-5);

&lt;/script&gt;

</description>
        <pubDate>2015-05-21 05:00:00 -0700</pubDate>
        <link>http://lakshgupta.github.io/2015/05/21/ArtificialNeuron/</link>
        <guid isPermaLink="true">http://lakshgupta.github.io/2015/05/21/ArtificialNeuron/</guid>
        
        
      </item>
    
  </channel>
</rss>
