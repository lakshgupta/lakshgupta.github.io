{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2 class=\"section-heading\">The Problem</h2>\n",
    "\n",
    "Single neuron has limited computational power and hence we need a way to build a network of neurons to make a more complex model. In this post we will look into how to construct a neural network and try to solve the handwritten digit recognition problem. The goal is to decide which digit it represents when given a new image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Understanding the Data</h2>\n",
    "\n",
    "We'll use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Luckily, [John Myles White](https://github.com/johnmyleswhite/MNIST.jl) has already created a package to import this dataset in Julia. The MNIST dataset provides a training set of 60,000 handwritten digits and a test set of 10,000 handwritten digits. Each of the image has a size of 28×28 pixels. ![MNIST](files/img/MNIST_digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Cloning MNIST from https://github.com/johnmyleswhite/MNIST.jl.git\n",
      "INFO: Computing changes...\n",
      "INFO: No packages to install, update or remove\n",
      "INFO: Package database updated\n",
      "INFO: Nothing to be done\n",
      "INFO: METADATA is out-of-date — you may not have the latest version of PyPlot\n",
      "INFO: Use `Pkg.update()` to get the latest versions of your packages\n"
     ]
    }
   ],
   "source": [
    "#Pkg.update();\n",
    "Pkg.clone(\"https://github.com/johnmyleswhite/MNIST.jl.git\");\n",
    "Pkg.add(\"PyPlot\")\n",
    "#Pkg.installed();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For plotting, PyPlot is a good option. It provides a Julia interface to the Matplotlib plotting library from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MNIST\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: int(x) is deprecated, use Int(x) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " in depwarn at ./deprecated.jl:63\n",
      " in int at deprecated.jl:49\n",
      " in anonymous at /home/vagrant/.julia/v0.4/MNIST/src/MNIST.jl:23\n",
      " in open at iostream.jl:114\n",
      " in traindata at /home/vagrant/.julia/v0.4/MNIST/src/MNIST.jl:58\n",
      " in include_string at loading.jl:153\n",
      " in execute_request_0x535c5df2 at /home/vagrant/.julia/v0.3/IJulia/src/execute_request.jl:157\n",
      " in eventloop at /home/vagrant/.julia/v0.3/IJulia/src/IJulia.jl:123\n",
      " in anonymous at task.jl:365\n",
      "while loading In[2], in expression starting on line 4\n"
     ]
    }
   ],
   "source": [
    "# ===================\n",
    "# load training data\n",
    "# ===================\n",
    "X,y = traindata();\n",
    "X=X';\n",
    "m = size(X, 1); # number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Training a model</h2>\n",
    "\n",
    "We want to train a neural network with one input layer, one hidden layer and one output layer to recognize handwritten digits. Since the dataset contains 28×28 pixel images, our neural network will have $28*28=784$ input neurons, a variable number of hidden neurons and $10$ output neurons.\n",
    "\n",
    "![2-layer-neuralNetwork](files/img/300px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# network setup\n",
    "# ======================\n",
    "inputLayerSize = size(X,2); # number of input features\n",
    "hiddenLayerSize = 25; # variable\n",
    "outputLayerSize = 10; # number of output classes\n",
    "\n",
    "# since we are doing multiclass classification: more than one output neurons\n",
    "# representing each output as an array of size of the output layer\n",
    "eyeY = eye(outputLayerSize);\n",
    "intY = [convert(Int64,i)+1 for i in y];\n",
    "Y = Array(Int64,length(y),outputLayerSize);\n",
    "for i = 1:m\n",
    "    Y[i,:] = eyeY[intY[i],:];\n",
    "end\n",
    "\n",
    "# randomly initializing weights\n",
    "epsilon_init = 0.12;\n",
    "# including one bias neuron in input layer\n",
    "# weights for the links connecting input layer to the hidden layer\n",
    "Theta1 = rand(hiddenLayerSize, inputLayerSize+1)* 2 * epsilon_init - epsilon_init;\n",
    "# including one bias neuron in hidden layer\n",
    "# weights for the links connecting hidden layer to the output layer\n",
    "Theta2 = rand(outputLayerSize, hiddenLayerSize+1)* 2 * epsilon_init - epsilon_init;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appraoch to train a neural network is similar to what we have discussed in the [neuron post](http://lakshgupta.github.io/2015/05/21/ArtificialNeuron/).  But since now we have a network of neurons, the way we follow the steps is a bit different. We'll use the [backpropagation algorithm](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf). \n",
    "\n",
    ">Backpropagation works by approximating the non-linear relationship between the input and the output by adjusting the weight values internally. \n",
    "The operations of the Backpropagation neural networks can be divided into two steps: feedforward and Backpropagation. In the feedforward step, an input pattern is applied to the input layer and its effect propagates, layer by layer, through the network until an output is produced. The network's actual output value is then compared to the expected output, and an error signal is computed for each of the output nodes. Since all the hidden nodes have, to some degree, contributed to the errors evident in the output layer, the output error signals are transmitted backwards from the output layer to each node in the hidden layer that immediately contributed to the output layer. This process is then repeated, layer by layer, until each node in the network has received an error signal that describes its relative contribution to the overall error.\n",
    "Once the error signal for each node has been determined, the errors are then used by the nodes to update the values for each connection weights until the network converges to a state that allows all the training patterns to be encoded. \n",
    "<p align=\"right\">- [www.cse.unsw.edu.au](http://www.cse.unsw.edu.au/~cs9417ml/MLP2/BackPropagation.html)</p>\n",
    "\n",
    "We'll discuss more about the backpropagation algorithm later but first let's collect the simple tools which are required for training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 class=\"section-heading\">Activation Function: $g$</h4>\n",
    "\n",
    "Considering [linear regression](http://lakshgupta.github.io/2015/05/27/LinearRegression/), using a linear activation function does not give us much advantage here. Linear function applied to a linear function is itself a linear function, and hence both the functions can be replaced by a single linear function. Moreover real world problems are generally more complex. A linear activation function may not be a good fit for the dataset we have. Therefore if the data we wish to model is non-linear then we need to account for that in our model. Sigmoid activation function is one of the reasonably good non-linear activation functions which we could use in our neural network.\n",
    "\n",
    "$$sigmoid(z) = 1/(1 + e^{-z})$$\n",
    "\n",
    "![sigmoid](files/img/400px-SigmoidFunction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================\n",
    "# activation function: computes the sigmoid of z \n",
    "# z is the weighted sum of inputs\n",
    "# ==============================================\n",
    "function sigmoid(z)\n",
    "    g = 1.0 ./ (1.0 + exp(-z));\n",
    "    return g;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 class=\"section-heading\">Cost Function: $J$</h4>\n",
    "\n",
    "The “squared error” cost function used for linear regression is not suitable for neural networks, because the non-linear nature of activation function is said to produce a non-convex plot of the cost function, i.e. with many local minima. For further explanation refer to this [thread](https://class.coursera.org/ml-005/forum/thread?thread_id=2773).\n",
    "\n",
    "Therefore we will use the cross-entropy cost function. \n",
    ">the cross-entropy is positive, and tends toward zero as the neuron gets better at computing the desired output, y, for all training inputs, x.\n",
    "<p align=\"right\">- [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html)</p>\n",
    "\n",
    "Take a look at this [paper](https://www-i6.informatik.rwth-aachen.de/publications/download/861/GolikPavelDoetschPatrickNeyHermann--Cross-Entropyvs.SquaredErrorTrainingaTheoreticalExperimentalComparison--2013.pdf) to have a comparison between cross entropy cost function and squared error cost function. So considering:\n",
    "$$J(\\theta) = \\frac{1}{m}(\\sum_{i=1}^{m}cost(h_{\\theta}(x^{(i)}),y^{(i)}))$$\n",
    "where:\n",
    "- $h_{\\theta}(x^{(i)})$ is the predicted value (hypothesis)\n",
    "- $y^{(i)}$ is the actual value (truth), and\n",
    "\\begin{eqnarray}\n",
    "cost(h_{\\theta}(x^{(i)}),y^{(i)})&=&\\left\\{\n",
    "\\begin{array}{l l}      \n",
    "    -\\log(h_{\\theta}(x^{(i)}))   &   \\mathrm{if} \\: y=1 \\\\\n",
    "    -\\log(1-h_{\\theta}(x^{(i)})) &  \\mathrm{if}  \\: y=0\n",
    "\\end{array}\\right.,  \\: h_{\\theta}(x^{(i)})\\in(0,1) \\\\ \\nonumber\n",
    "&=& - y^{(i)}\\log{h_{\\theta}(x^{(i)})} - (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\n",
    "\\end{eqnarray}\n",
    "\n",
    "Hence our cost function becomes:\n",
    "$$J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^{m} ( y^{(i)}\\log{h_{\\theta}(x^{(i)})} + (1-y^{(i)})\\log({1-h_{\\theta}(x^{(i)}))})]$$\n",
    "\n",
    "We don't sum over the bias terms hence starting at 1 for the summation. But this cost function will work if we have a single neuron in the output layer. Let's generalize this cost function so that we could use it for $k$ neurons in the output layer. \n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{i=1}^{k}[ y^{(i)}_k\\log{(h_{\\theta}(x^{(i)})_k)} + (1-y^{(i)}_k)\\log({1-(h_{\\theta}(x^{(i)}))_k)}]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 class=\"section-heading\">Regularization: $L^2$</h4>\n",
    "\n",
    "Most regularization approaches add a parameter norm penalty $\\Omega(\\theta)$ to the loss function $J$ to achieve better generalization of the model. In case of $L^2$ regularization, also known as weight decay, the penalty is equal to the sum of the square of all of the weight vectors.\n",
    "\n",
    "$$\\Omega(\\theta) = \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}((\\theta_{ji}^l)^2)$$ \n",
    "\n",
    "where \n",
    "- $\\lambda>0$, is known as the regularization parameter\n",
    "- $m$ is the size of our training set\n",
    "- $L$ in the equation is the layer number\n",
    "- $s$ is the neuron unit in the corresponding layer\n",
    "\n",
    ">Regularizers work by trading increased bias for reduced variance. An effective regularizer is one that makes a proﬁtable trade, that is it reduces variance signiﬁcantly while not overly increasing the bias.\n",
    "<p align=\"right\">- [Yoshua Bengio, Ian Goodfellow and Aaron Courville](http://www.iro.umontreal.ca/~bengioy/dlbook/regularization.html)</p>\n",
    "\n",
    "The Wikipedia has a descent article on the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "costFunction (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight regularization parameter\n",
    "lambda = 3; \n",
    "\n",
    "# ===============================================\n",
    "# cross entropy cost function with regularizarion\n",
    "# ===============================================\n",
    "function costFunction(prediction, truth)\n",
    "    cost = zeros(m,1); # cost for each input\n",
    "    for i=1:m\n",
    "        cost[i,:] = (-truth[i,:]*log(prediction[i,:]')) - ((1-truth[i,:])*log(1-prediction[i,:]'));\n",
    "    end\n",
    "\n",
    "    # regularization term\n",
    "    regularization = (lambda/(2*m))*(sum(sum(Theta1[:,2:end].^2)) + sum(sum(Theta2[:,2:end].^2)));\n",
    "\n",
    "    return (1/m)*sum(cost) + regularization; # regularized cost\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 class=\"section-heading\">Backpropagation</h4>\n",
    "\n",
    "Despite the name, [backpropagation algorithm](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm) consist of two phases:\n",
    "- Feedforward \n",
    "- Backpropagation\n",
    "\n",
    "The feedforward process is the same process we have been following in the previous posts. Using the feedforward process we calculate the weighted sum of inputs and apply the activation function to get an output as we move from layer to layer. In the end we come up with a output activation which could have some error as compared to the actual values. To have the output as close as possible to the actual values we use the backpropagation process to tune the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a way of computing gradients of expressions through recursive application of chain rule. We start from the output layer and go backwards calulating the gradient on the activations for each layer till the first hidden layer.\n",
    "\n",
    ">From these gradients, which can be interpreted as an indication of how each layer’s output should change to reduce error, one can obtain the gradient on the parameters of each layer. The gradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients havebeen computed) or used with other gradient-based optimization methods.\n",
    "<p align=\"right\">- [Yoshua Bengio, Ian Goodfellow and Aaron Courville](http://www.iro.umontreal.ca/~bengioy/dlbook/regularization.html)</p>\n",
    "\n",
    "Since we are learning/tuning weights ($\\theta$), we want to evaluate: $\\dfrac{\\partial E}{\\partial \\theta_{ij}^{l}}$\n",
    "\n",
    "here,\n",
    "$$ \\theta_{ij}^{l} =\n",
    "\\begin{cases}\n",
    "1 \\ge l \\ge L,  & \\text{layer} \\\\\n",
    "0 \\ge i \\ge d^{(l-1)}, & \\text{input} \\\\\n",
    "1 \\ge i \\ge d^{(l)}, & \\text{output}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Using the chain rule, we can solve the above partial derivative as:\n",
    "$$\\dfrac{\\partial E}{\\partial \\theta_{ij}^{l}} = \\underbrace{\\dfrac{\\partial E}{\\partial s_{j}^{l}}}_{1} \\underbrace{\\dfrac{\\partial s_{j}^{l}}{\\partial \\theta_{ij}^{l}}}_{2}$$\n",
    "\n",
    "here, $s$ represents the input signal to a neuron which is the weighted sum of the inputs or in other words weighted sum of the outputs from the previous layer's neurons. Hence $(2)$ becomes:\n",
    "\n",
    "$$\\dfrac{\\partial s_{j}^{l}}{\\partial \\theta_{ij}^{l}} = x^{(l-1)}$$\n",
    "\n",
    "where, $x$ is the value from applying activation function $g$ to $s$. Now let's look at $(1)$ and represent it as $\\delta$. Since we start backpropagation from the last output layer, we can numerically calculate it as:\n",
    "$$\\delta_{j}^{l} = \\dfrac{\\partial E}{\\partial s_{j}^{l}} = error(x^{l},y)$$\n",
    "\n",
    "$error$ could be any function to tell us how much far off we are from the actual value. For the MNIST problem, we'll simply subtract the predicted values from the actual values. \n",
    "\n",
    "For the hidden layers, we can break down $\\delta$ as:\n",
    "\\begin{eqnarray}\n",
    "\\delta_{i}^{l-1}&=& \\dfrac{\\partial E}{\\partial s_{i}^{l-1}} \\\\\n",
    "&=& \\sum_{j=1}^{d^{(l)}} (\\dfrac{\\partial E}{\\partial s_{j}^{l}}) (\\dfrac{\\partial s_{j}^{l}}{\\partial x_{i}^{l-1}}) (\\dfrac{\\partial x_{i}^{l-1}}{\\partial s_{i}^{l-1}}) \\\\\n",
    "&=& \\sum_{j=1}^{d^{(l)}} [(\\delta_{j}^{l}) (\\theta_{ij}^{l}) (g'(s_i^{l-1}))]\n",
    "\\end{eqnarray}\n",
    "\n",
    "We have now all the components for $\\dfrac{\\partial E}{\\partial \\theta_{ij}^{l}}$, hence we can update the weights as:\n",
    "$$\\theta_{ij}^{(l)} \\leftarrow \\theta_{ij}^{(l)} - \\alpha (\\delta_{j}^{l} x_{i}^{l-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoidGradient (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the gradient of the sigmoid function evaluated at z\n",
    "function sigmoidGradient(z)\n",
    "  return sigmoid(z).*(1-sigmoid(z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate\n",
    "alpha = 0.1;\n",
    "# number of iterations\n",
    "epoch = 100;\n",
    "# cost per epoch\n",
    "J = zeros(epoch,1);\n",
    "# ====================================================================\n",
    "# Train the neural network using feedforward-backpropagation algorithm\n",
    "# ====================================================================\n",
    "for i = 1:epoch\n",
    "    # ===================\n",
    "    # Feedforward process\n",
    "    # ===================\n",
    "    # input layer\n",
    "    # add one bias element\n",
    "    x1 = [ones(size(X,1),1) X];\n",
    "\n",
    "    # hidden layer\n",
    "    s2 = x1*Theta1';\n",
    "    x2 = sigmoid(s2);\n",
    "    # add one bias element\n",
    "    x2 = [ones(size(x2,1),1) x2];\n",
    "\n",
    "    # output layer\n",
    "    s3 = x2*Theta2';\n",
    "    x3 = sigmoid(s3);\n",
    "    \n",
    "    # ===========================\n",
    "    # find cost of the new output\n",
    "    # ===========================\n",
    "    J[i,:] = costFunction(x3, Y);\n",
    "        \n",
    "    # =======================\n",
    "    # Backpropagation process\n",
    "    # =======================\n",
    "    inputLayerRegularizedGradient = zeros(size(Theta1));\n",
    "    hiddenLayerRegularizedGradient = zeros(size(Theta2));\n",
    "\n",
    "    # delta for output layer\n",
    "    delta3 = x3 - Y;\n",
    "    \n",
    "    # gradient for hidden layer\n",
    "    theta2_regularization = ((lambda/m)*Theta2);\n",
    "    theta2_regularization[:,1] = 0;\n",
    "    hiddenLayerRegularizedGradient = (delta3'*x2)/m #+ theta2_regularization;\n",
    "    # delta for hidden layer\n",
    "    delta2 = (delta3*Theta2[:, 2:end]).*sigmoidGradient(s2) ;\n",
    "\n",
    "    # gradient for input layer\n",
    "    theta1_regularization = ((lambda/m)*Theta1);\n",
    "    theta1_regularization[:,1] = 0;\n",
    "    inputLayerRegularizedGradient = (delta2'*x1)/m #+ theta1_regularization;\n",
    "    # there is no delta term for the input layer\n",
    "        \n",
    "    # adjust the weights (thetas)\n",
    "    Theta1 = Theta1 - alpha* inputLayerRegularizedGradient;\n",
    "    Theta2 = Theta2 - alpha* hiddenLayerRegularizedGradient;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our implementation is correct, the cost of the predicted output after each iteration should drop. I'll cover another method (gradient check) in another post to validate our implementation. But for now let's check by plotting the cost per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cost per iteration\n",
    "plot(1:length(J), J)\n",
    "xlabel(\"Iterations\")\n",
    "ylabel(\"Cost\")\n",
    "grid(\"on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Prediction and Accuracy</h2>\n",
    "\n",
    "After training the model we'll check how well our model has learned all the weights to make a prediction. So we'll evaluate the performance on the train dataset first. We do it by following a way similar to the feedforward process. Consider that we now do not have randonly initialized weights but rather obtained after going through the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "# outputs the predicted label of X given the\n",
    "# trained weights of a neural network (Theta1, Theta2)\n",
    "# Similar to feedforward process.\n",
    "##########################################################################\n",
    "function predict(Theta1, Theta2, X)\n",
    "    m = size(X, 1); # size of the data\n",
    "    p = zeros(size(X, 1), 1); # to save our prediction\n",
    "    h1 = sigmoid([ones(m, 1) X] * Theta1'); # hidded layer output\n",
    "    h2 = sigmoid([ones(m, 1) h1] * Theta2'); # output layer\n",
    "    # find the index with the max value in the array of size 10\n",
    "    # subtract 1 from the index since we are using 1 to \n",
    "    # represent 0, 2 for 3 and so on (while calculating yInter)\n",
    "    for i=1:m\n",
    "        p[i,:] = indmax(h2[i,:])-1;\n",
    "      end\n",
    " return p;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make prediction\n",
    "pred = predict(Theta1, Theta2, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################\n",
    "# calculate the accuracy of the prediction\n",
    "###############################################\n",
    "function accuracy(truth, prediction)\n",
    "    m = length(truth);\n",
    "    if m!=length(prediction)\n",
    "        error(\"truth and prediction length mismatch\");\n",
    "    end\n",
    "    \n",
    "    # calculate the % of predicted values\n",
    "    # matching the actual values\n",
    "    sum =0;\n",
    "    for i=1:m\n",
    "        if truth[i,:] == pred[i,:]\n",
    "            sum = sum +1;\n",
    "        end\n",
    "    end\n",
    "  return (sum/m)*100;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 9.93\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "println(\"train accuracy: \", accuracy(y, pred));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the accuracy on the train dataset, let's check the accuracy on the test dataset to be sure that we did not overfit the data. If there is too much difference then we might have to tune some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===============\n",
    "# load test data\n",
    "# ===============\n",
    "XTest,yTest = testdata();\n",
    "XTest=XTest';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make prediction\n",
    "predTest = predict(Theta1, Theta2, XTest);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 9.66\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "println(\"test accuracy: \", accuracy(yTest, predTest));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! Now our model can make the prediction on any new handwritten digit in a similar way as we made the prediction on the test dataset. There are different ways to improve the model but I'll leave them for the coming posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">References:</h2>\n",
    "\n",
    "- [Deep Learning in Neural Networks: An Overview](http://arxiv.org/abs/1404.7828)\n",
    "- [Learning representations by back-propagating errors](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)\n",
    "- [Coursera Machine Learning](https://class.coursera.org/ml-005)\n",
    "- [CS 231n](http://cs231n.github.io/optimization-2/)\n",
    "- [Deep Learning](http://www.iro.umontreal.ca/~bengioy/dlbook/mlp.html)\n",
    "- [UDFL](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm)\n",
    "- [Learning from Data](http://work.caltech.edu/slides/slides10.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
