{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install\n",
    "#Pkg.add(\"MNIST\");\n",
    "using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "X,y = traindata(); \n",
    "m = size(X, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLayerSize = size(X,1); \n",
    "hiddenLayerSize = 25;\n",
    "outputLayerSize = 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# representing each output as an array of size of the output layer\n",
    "Y = zeros(outputLayerSize, m);\n",
    "for i = 1:m\n",
    "    Y[y[i]+1,i] = 1;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(z)\n",
    "    g = 1.0 ./ (1.0 + exp(-z));\n",
    "    return g;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoidGradient (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoidGradient(z)\n",
    "  return sigmoid(z).*(1-sigmoid(z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "costFunction (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight regularization parameter\n",
    "lambda = 2; \n",
    "function costFunction(truth, prediction)\n",
    "    #cost\n",
    "    cost = zeros(m,1);\n",
    "    for i=1:m\n",
    "        cost[i,:] = (-Y[:,i]'*log(prediction[:,i])) - ((1-Y[:,i]')*log(1-prediction[:,i]));\n",
    "    end\n",
    "    # regularization term\n",
    "    regularization = (lambda/(2*m))*(sum(sum(Theta1[2:end,:].^2)) + sum(sum(Theta2[2:end,:].^2)));\n",
    "\n",
    "    return (1/m)*sum(cost) + regularization; # regularized cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(Theta1Wt, Theta2Wt, data)\n",
    "    dataSize = size(data, 2); \n",
    "    p = zeros(dataSize, 1);\n",
    "    h1 = sigmoid(Theta1Wt'*[ones(1,size(data,2)), data]);\n",
    "    h2 = sigmoid(Theta2Wt'*[ones(1,size(h1,2)), h1]);\n",
    "    # 1 index is for 0, 2 for 1 ...so forth\n",
    "    for i=1:dataSize\n",
    "        p[i,:] = indmax(h2[:,i])-1;\n",
    "    end\n",
    "    return p;\n",
    "end\n",
    "\n",
    "function accuracy(truth, prediction)\n",
    "    dataSize = length(truth);\n",
    "    match =0;\n",
    "    for i=1:dataSize\n",
    "        if truth[i,:] == pred[i,:]\n",
    "            match = match +1;\n",
    "        end\n",
    "    end\n",
    "    return (match/dataSize)*100;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# including one bias neuron in input layer\n",
    "# weights for the links connecting input layer to the hidden layer\n",
    "Theta1 = randn(inputLayerSize+1, hiddenLayerSize); #(785x25)\n",
    "# including one bias neuron in hidden layer\n",
    "# weights for the links connecting hidden layer to the output layer\n",
    "Theta2 = randn(hiddenLayerSize+1, outputLayerSize); #(26x10)\n",
    "# learning rate\n",
    "alpha = 0.1;\n",
    "# number of iterations\n",
    "epoch = 1000;\n",
    "# cost per epoch\n",
    "J = zeros(epoch,1);\n",
    "# ====================================================================\n",
    "# Train the neural network using feedforward-backpropagation algorithm\n",
    "# ====================================================================\n",
    "for i = 1:epoch\n",
    "    Delta1 = 0;\n",
    "    Delta2 = 0;\n",
    "    for j = 1:m # for each input\n",
    "        # ===================\n",
    "        # Feedforward process\n",
    "        # ===================\n",
    "        # input layer\n",
    "        # add one bias element\n",
    "        x1 = [1, X[:,j]];\n",
    "\n",
    "        # hidden layer\n",
    "        s2 = Theta1'*x1;\n",
    "        x2 = sigmoid(s2);\n",
    "        # add one bias element\n",
    "        x2 = [1, x2];\n",
    "\n",
    "        # output layer\n",
    "        s3 = Theta2'*x2;\n",
    "        x3 = sigmoid(s3);\n",
    "        \n",
    "        # =======================\n",
    "        # Backpropagation process\n",
    "        # =======================\n",
    "        # delta for output layer\n",
    "        delta3 = x3 - Y[:,j];\n",
    "        delta2 = (Theta2[2:end,:]*delta3).*sigmoidGradient(s2) ;\n",
    "        # there is no delta term for the input layer\n",
    "        \n",
    "        # adjust the weights (thetas)\n",
    "        Delta1 = Delta1 + x1*delta2';\n",
    "        Delta2 = Delta2 + x2*delta3';\n",
    "    end\n",
    "    \n",
    "    reg_theta1 = ((lambda/m)*Theta1);\n",
    "    reg_theta1[1,:] = 0;\n",
    "    Theta1 = Theta1 - ((alpha/m)*Delta1 + reg_theta1);\n",
    "    \n",
    "    reg_theta2 = ((lambda/m)*Theta2);\n",
    "    reg_theta2[1,:] = 0;\n",
    "    Theta2 = Theta2 - ((alpha/m)* Delta2 + reg_theta2);\n",
    "    \n",
    "    h1 = sigmoid(Theta1'*[ones(1,size(X,2)), X]);\n",
    "    h2 = sigmoid(Theta2'*[ones(1,size(h1,2)), h1]);\n",
    "    J[i,:] = costFunction(Y, h2);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20x1 Array{Float64,2}:\n",
       " 12.669  \n",
       " 12.4574 \n",
       " 12.2516 \n",
       " 12.0514 \n",
       " 11.8556 \n",
       " 11.6646 \n",
       " 11.477  \n",
       " 11.2909 \n",
       " 11.1161 \n",
       " 10.9428 \n",
       " 10.7719 \n",
       " 10.608  \n",
       " 10.4477 \n",
       " 10.292  \n",
       " 10.1387 \n",
       "  9.98914\n",
       "  9.84468\n",
       "  9.70279\n",
       "  9.56375\n",
       "  9.43196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 13.686666666666666\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X);\n",
    "println(\"train accuracy: \", accuracy(y, pred));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
