{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will walk you through the process of implementing \n",
    "\n",
    "- A softmax function\n",
    "- A simple neural network\n",
    "- Back propagation\n",
    "- Word2vec models\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD) for a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax\n",
    "\n",
    ">If you want the outputs of a network to be interpretable as posterior\n",
    ">probabilities for a categorical target variable, it is highly desirable for\n",
    ">those outputs to lie between zero and one and to sum to one. The purpose of\n",
    ">the softmax activation function is to enforce these constraints on the\n",
    ">outputs. \n",
    "\n",
    "http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html\n",
    "\n",
    "\n",
    "$$softmax(x) = softmax(x + c)$$\n",
    "where $x + c$ means adding the constant $c$ to every dimension of $x$.\n",
    "\n",
    "Note: In practice, we make use of this property and choose $c = âˆ’ max_ix_i$ when computing softmax probabil-\n",
    "ities for numerical stability (i.e. subtracting its maximum element from all elements of x).\n",
    "\n",
    ">Hence you can always pick one of the output units, and\n",
    "add an appropriate constant to each net input to produce any desired net\n",
    "input for the selected output unit, which you can choose to be zero or\n",
    "whatever is convenient. You can use the same trick to make sure that none of\n",
    "the exponentials overflows.\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x)\n",
    "    # Softmax function #\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.\n",
    "    #\n",
    "    # http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression\n",
    "    ###################################################################\n",
    "    ### YOUR CODE HERE\n",
    "    # find max element per row\n",
    "    row = size(x,1);\n",
    "    xMax = zeros(size(x));\n",
    "    for r = 1:row\n",
    "        xMax[r,:] = exp(x[r,:] - maximum(x[r,:]))/sum(exp(x[r,:]-maximum(x[r,:]))) ;\n",
    "    end\n",
    "    x = xMax;\n",
    "    ### END YOUR CODE\n",
    "    return x;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[0.2689414213699951 0.7310585786300049\n",
      " 0.2689414213699951 0.7310585786300049]\n",
      "[0.2689414213699951 0.7310585786300049\n",
      " 0.2689414213699951 0.7310585786300049]\n",
      "[0.7310585786300049 0.2689414213699951]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "println(\"=== For autograder ===\");\n",
    "println(softmax([[1 2];[3 4]]));\n",
    "println(softmax([[1001 1002];[3 4]]));\n",
    "println(softmax([[-1001 -1002]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "In this part, we're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(x)\n",
    "    # Sigmoid function #\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1.0/(1.0+exp(-x));\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid_grad (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid_grad(f)\n",
    "    # Sigmoid gradient function #\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = sigmoid(f.*(1.0-f);\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[0.7310585786300049 0.8807970779778823\n",
      " 0.2689414213699951 0.11920292202211755]\n",
      "[0.19661193324148185 0.10499358540350662\n",
      " 0.19661193324148185 0.1049935854035065]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = [[1 2], [-1 -2]];\n",
    "f = sigmoid(x);\n",
    "g = sigmoid_grad(f);\n",
    "println(\"=== For autograder ===\");\n",
    "println(f);\n",
    "println(g);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the functions implemented above to implement a neural network with one sigmoid hidden layer. \n",
    "\n",
    "To calculate the numerical gradient we'll use:\n",
    "$$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x - h)}{2h} \\hspace{0.1in}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradcheck_naive (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "function gradcheck_naive(f, x)\n",
    "    ###\n",
    "    # Gradient check for a function f \n",
    "    # - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    # - x is the point (numpy array) to check the gradient at\n",
    "    ### \n",
    "\n",
    "    #rndstate = random.getstate()\n",
    "    #random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    for ix in eachindex(grad)\n",
    "        #println(x[ix])\n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        fxBefore = f(x[ix]-h)[1];\n",
    "        fxAfter = f(x[ix]+h)[1];\n",
    "        numgrad = (fxAfter - fxBefore)/(2*h);\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if(reldiff > 1e-5)\n",
    "            println(\"Gradient check failed.\")\n",
    "            println(\"First gradient error found at index \",ix)\n",
    "            println(\"Your gradient: \", grad[ix] ,\" \\t Numerical gradient:\", numgrad)\n",
    "            return\n",
    "        end\n",
    "    \n",
    "    end\n",
    "        println(\"Gradient check passed!\");\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quad (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function quad(x)\n",
    "    return sum(x.^2),x*2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "#quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "println(\"=== For autograder ===\")\n",
    "gradcheck_naive(quad, collect(123.456))      # scalar test\n",
    "gradcheck_naive(quad, randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115-element Array{Float64,1}:\n",
       " -2.72695   \n",
       "  0.612086  \n",
       " -0.0590889 \n",
       " -1.36927   \n",
       " -0.00648123\n",
       "  1.34737   \n",
       "  1.92513   \n",
       "  0.0629283 \n",
       "  0.46731   \n",
       " -1.29867   \n",
       "  1.0087    \n",
       " -0.0265673 \n",
       " -0.0486206 \n",
       "  â‹®         \n",
       " -0.360809  \n",
       "  0.277802  \n",
       " -0.521234  \n",
       "  0.759818  \n",
       "  0.325637  \n",
       " -0.477799  \n",
       "  0.514196  \n",
       " -1.6942    \n",
       " -0.319777  \n",
       " -1.26717   \n",
       "  0.443363  \n",
       " -0.585058  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = randn(N, dimensions[1])   # each row will be a datum\n",
    "labels = zeros((N, dimensions[3]))\n",
    "for i=1:N\n",
    "    labels[i, rand(1:dimensions[3])] = 1\n",
    "end\n",
    "params = randn((dimensions[1] + 1) * dimensions[2] + (dimensions[2] + 1) * dimensions[3], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_backward_prop (generic function with 1 method)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_backward_prop(data, labels, params)\n",
    "    #\"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 1\n",
    "    W1 = reshape(params[t:dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b1 = reshape(params[t:t-1+dimensions[2]], (1, dimensions[2]))\n",
    "    t += dimensions[2]\n",
    "    W2 = reshape(params[t:t-1+dimensions[2]*dimensions[3]], (dimensions[2], dimensions[3]))\n",
    "    t += dimensions[2]*dimensions[3]\n",
    "    b2 = reshape(params[t:t-1+dimensions[3]], (1, dimensions[3]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    N, D = size(data)\n",
    "    \n",
    "    \n",
    "    z1 = (data*W1).+b1\n",
    "    activation1 = sigmoid(z1)\n",
    "    scores = softmax(activation1*W2 .+ b2)\n",
    "    #cost = sum(- log(scores[labels == 1])) / N\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "\n",
    "    #dscores = scores - labels\n",
    "    \n",
    "    #dscores /= N\n",
    "    \n",
    "   # gradb2 = np.sum(dscores, axis=0)\n",
    "   # gradW2 = np.dot(h.T, dscores)\n",
    "    \n",
    "    \n",
    "    #grad_h = np.dot(dscores, W2.T)\n",
    "    #grad_h = sigmoid_grad(h) * grad_h\n",
    "    \n",
    "    #gradb1 = np.sum(grad_h, axis=0)\n",
    "    #gradW1 = np.dot(data.T, grad_h)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    #grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    #return cost, grad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20x10 Array{Float64,2}:\n",
       " 0.05982     0.144687   0.482001  â€¦  0.00900514  0.0316486   0.00555769\n",
       " 0.0203127   0.50733    0.139886     0.013758    0.0312936   0.0335686 \n",
       " 0.0832262   0.244695   0.359195     0.0024399   0.0335761   0.00445099\n",
       " 0.0431302   0.413596   0.159337     0.00932879  0.0531728   0.0302763 \n",
       " 0.0234169   0.24347    0.336009     0.0101099   0.0118017   0.0133297 \n",
       " 0.0693191   0.366145   0.219871  â€¦  0.00471082  0.0540632   0.0131887 \n",
       " 0.00953332  0.0672128  0.648148     0.0218245   0.00204121  0.00337138\n",
       " 0.0764868   0.118373   0.498219     0.00773054  0.0327679   0.00441254\n",
       " 0.00977199  0.213009   0.461104     0.0213747   0.00841669  0.00668187\n",
       " 0.0505441   0.388085   0.221576     0.00871854  0.0444769   0.0180464 \n",
       " 0.0141623   0.390797   0.299265  â€¦  0.0181372   0.0105226   0.0145866 \n",
       " 0.00378141  0.0890384  0.516085     0.00609009  0.00181097  0.00157955\n",
       " 0.0198691   0.403058   0.216321     0.0198355   0.0297633   0.0249765 \n",
       " 0.0640671   0.394918   0.185823     0.00587248  0.0636349   0.0182199 \n",
       " 0.0837464   0.161011   0.44775      0.00342245  0.0325771   0.00367183\n",
       " 0.0664835   0.372294   0.20566   â€¦  0.00643564  0.0620791   0.0170993 \n",
       " 0.0169086   0.351943   0.298195     0.0221271   0.0164677   0.0179072 \n",
       " 0.110937    0.306235   0.180314     0.0024226   0.11372     0.0106886 \n",
       " 0.0426967   0.0585877  0.674377     0.00391904  0.0102842   0.00123172\n",
       " 0.0845404   0.233123   0.383206     0.00237399  0.0291285   0.00371747"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "println(\"=== For autograder ===\")\n",
    "forward_backward_prop(data, labels, params)\n",
    "#gradcheck_naive(forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
