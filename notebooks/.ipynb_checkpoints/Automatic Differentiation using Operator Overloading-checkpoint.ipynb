{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the libraries for creating neural networks (Tensorflow, Theano, Torch, etc) are using automatic differentiation (AD) in one way or another. It has applications in the other parts of the mathematical world as well since it is a clever and effective way to calculate the gradients, effortlessly. It works by first creating a computational graph of the operations and then traversing it in either forward mode or reverse mode. Let's see an implementation using operator overloading for each of the modes to calculate the first order derivative. I'll be using the same examples as used in the Colah's blog [here](http://colah.github.io/posts/2015-08-Backprop/). I highly recommned reading it first. Since that article already has an excellent explanation I'll be mainly focusing on the implementation part. It may not be the best performing piece of code for AD but I think it's the simplest for getting your head around the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Base.+, Base.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Forward Mode</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type ADFwd\n",
    "    value::Float64\n",
    "    derivative::Float64\n",
    "    \n",
    "    ADFwd(val::Float64) = new(val, 0)\n",
    "    ADFwd(val::Float64, der::Float64) = new(val, der)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function adf_add(x::ADFwd, y::ADFwd)\n",
    "    return ADFwd(x.value + y.value, x.derivative + y.derivative)\n",
    "end\n",
    "+(x::ADFwd, y::ADFwd) = adf_add(x, y)\n",
    "\n",
    "function adf_mul(x::ADFwd, y::ADFwd)\n",
    "    return ADFwd(x.value * y.value, y.value * x.derivative + x.value * y.derivative)\n",
    "end\n",
    "*(x::ADFwd, y::ADFwd) = adf_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function testForwardMode(x::ADFwd,y::ADFwd)\n",
    "    (x+y)*(y + ADFwd(1.0))\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the partial derivative of testForwardMode with respect to x. To do this, we will need to pass in a unit vector pointing along the x axis as the increment for evaluating the Jaobian against, therefore we pass in 1 when creating the ADFwd for x, and 0 for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwd = ADFwd(2.0, 1.0)\n",
    "yFwd = ADFwd(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwdDer = testForwardMode(xFwd, yFwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwd.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yFwd.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwdDer.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do the same to calculate the derivative with respect to 'y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwd = ADFwd(2.0)\n",
    "yFwd = ADFwd(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yFwdDer = testForwardMode(xFwd, yFwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xFwd.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yFwd.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yFwdDer.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Reverse Mode</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type ADRev\n",
    "    value::Float64\n",
    "    derivative::Float64\n",
    "    derivativeOp::Function\n",
    "    parents::Array{ADRev}\n",
    "    \n",
    "    ADRev(val::Float64) = new(val, 0, ad_constD, Array(ADRev,0))\n",
    "    ADRev(val::Float64, der::Float64) = new(val, der, ad_constD, Array(ADRev,0))\n",
    "end\n",
    "\n",
    "function ad_constD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function adr_add(x::ADRev, y::ADRev)\n",
    "    result = ADRev(x.value + y.value)\n",
    "    result.derivativeOp = adr_addD\n",
    "    push!(result.parents, x)\n",
    "    push!(result.parents, y)\n",
    "    return result\n",
    "end\n",
    "function adr_addD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    adNodes[1].derivative = adNodes[1].derivative + prevDerivative * 1\n",
    "    adNodes[2].derivative = adNodes[2].derivative + prevDerivative * 1\n",
    "    return\n",
    "end\n",
    "+(x::ADRev, y::ADRev) = adr_add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function adr_mul(x::ADRev, y::ADRev)\n",
    "    result = ADRev(x.value * y.value)\n",
    "    result.derivativeOp = adr_mulD\n",
    "    push!(result.parents, x)\n",
    "    push!(result.parents, y)\n",
    "    return result\n",
    "end\n",
    "function adr_mulD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    adNodes[1].derivative = adNodes[1].derivative + prevDerivative * adNodes[2].value\n",
    "    adNodes[2].derivative = adNodes[2].derivative + prevDerivative * adNodes[1].value\n",
    "    return\n",
    "end\n",
    "*(x::ADRev, y::ADRev) = adr_mul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xRev = ADRev(2.0)\n",
    "yRev = ADRev(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function f(x::ADRev,y::ADRev)\n",
    "    (x+y)*(y + ADRev(1.0))\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function backprop(graph::ADRev)\n",
    "    current = graph\n",
    "    # set the derivative to 1\n",
    "    current.derivative = 1\n",
    "    bfs = [current]\n",
    "    while length(bfs) != 0\n",
    "        current = pop!(bfs)\n",
    "        currDerivative = current.derivative\n",
    "        current.derivativeOp(currDerivative, current.parents)\n",
    "        numParents = length(current.parents)\n",
    "        for i=1:numParents \n",
    "            push!(bfs, current.parents[i])\n",
    "        end\n",
    "    end\n",
    "    return graph\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fRev = backprop(f(xRev, yRev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xRev.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yRev.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 class=\"section-heading\">References:</h2>\n",
    "\n",
    "- [Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "- [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [ALGORITHMIC/AUTOMATIC DIFFERENTIATION](http://blog.tombowles.me.uk/2014/09/10/ad-algorithmicautomatic-differentiation/)\n",
    "- [Efficient Calculation of Derivatives using Automatic Differentiation](https://www.duo.uio.no/bitstream/handle/10852/41535/Kjelseth-Master.pdf?sequence=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
