{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the libraries for creating neural networks (Tensorflow, Theano, Torch, etc.) are using automatic differentiation (AD) in one way or another. It has applications in the other parts of the mathematical world as well since it is a clever and effective way to calculate the gradients, effortlessly. It works by first creating a computational graph of the operations and then traversing it in either forward mode or reverse mode. Let us see how to implement them using operator overloading to calculate the first order partial derivative. I highly recommend reading Colah's blog [here](http://colah.github.io/posts/2015-08-Backprop/) first. It has an excellent explanation about computational graphs and this post is related to the implementation side of it. It may not be the best performing piece of code for AD but I think it's the simplest one for getting your head around the concept. The example function we are considering here is:\n",
    "\n",
    "$$f(a, b) = (a + b) * (b + 1)$$\n",
    "\n",
    "Using basic differential calculus rules, we can calculate the derivative of the above function with respect to a and b by applying the [sum rule](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation) and the [product rule](https://en.wikipedia.org/wiki/Product_rule):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial e}{\\partial a}\n",
    "&=&(c*\\frac{\\partial d}{\\partial a}) + (\\frac{\\partial c}{\\partial a}*d) \\\\ \\nonumber\n",
    "&=&((a + b)* (\\frac{\\partial b}{\\partial a} + \\frac{\\partial 1}{\\partial a}) )+ ((\\frac{\\partial a}{\\partial a} + \\frac{\\partial b}{\\partial a})* (b + 1)) \\\\ \\nonumber \n",
    "&=&(1 + 0)* (b + 1)) \\\\ \\nonumber \n",
    "&=&b + 1 \\\\ \\nonumber \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and similarly, \n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial e}{\\partial b}\n",
    "&=&(c*\\frac{\\partial d}{\\partial b}) + (\\frac{\\partial c}{\\partial b}*d) \\\\ \\nonumber\n",
    "&=&((a + b)* (\\frac{\\partial b}{\\partial b} + \\frac{\\partial 1}{\\partial b})) + ((\\frac{\\partial a}{\\partial b} + \\frac{\\partial b}{\\partial b})* (b + 1)) \\\\ \\nonumber \n",
    "&=&((a + b)*(1 + 0)) * ((0 + 1)*(b + 1)) \\\\ \\nonumber \n",
    "&=&(a + b)*(b + 1) \\\\ \\nonumber \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "In order to get the derivative of a function programmatically there are two approaches we can follow and operate on the computational graph, forward mode and reverse mode. Both of these approaches make use of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for overloading\n",
    "import Base.+, Base.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Forward Mode</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode is very similar to the calculation we did above. We pick an independent variable with respect to which we would like to calculate the partial derivative of the function, set its derivative with respect to itself as 1 and then, we recursively moving forward calculate the derivative of the sub-graph till we reach the output node.\n",
    "\n",
    ">In a pen-and-paper calculation, one can do so by repeatedly substituting the derivative of the inner functions in the chain rule:\n",
    ">\n",
    "$${\\displaystyle {\\frac {\\partial y}{\\partial x}}={\\frac {\\partial y}{\\partial w_{1}}}{\\frac {\\partial w_{1}}{\\partial x}}={\\frac {\\partial y}{\\partial w_{1}}}\\left({\\frac {\\partial w_{1}}{\\partial w_{2}}}{\\frac {\\partial w_{2}}{\\partial x}}\\right)={\\frac {\\partial y}{\\partial w_{1}}}\\left({\\frac {\\partial w_{1}}{\\partial w_{2}}}\\left({\\frac {\\partial w_{2}}{\\partial w_{3}}}{\\frac {\\partial w_{3}}{\\partial x}}\\right)\\right)=\\cdots }$$ \n",
    "><p>- <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\">wikipedia</a></p>\n",
    "\n",
    "The graph here can be thought to be constructed by way a programming language may perform the operations, using the [BODMAS](https://en.wikipedia.org/wiki/Order_of_operations) rule. In terms of simple operations, the above function can be broken down to:\n",
    "\n",
    "$$c = a + b$$\n",
    "$$d = b + 1$$\n",
    "$$e = c * d$$\n",
    "\n",
    "hence the operations to calculate the partial derivative of the above function with respect to a may look like:\n",
    "\n",
    "\\begin{array}{cc|lcr|lcr}\n",
    "\\mathrm{value} && \\mathrm{derivative} && node\\\\\n",
    "\\hline \\\\\n",
    "a=a   && \\frac{\\partial a}{\\partial a} = 1  && node 1\\\\\n",
    "b=b   && \\frac{\\partial b}{\\partial a} = 0  && node 2\\\\\n",
    "c=a+b && \\frac{\\partial c}{\\partial a} = \\frac{\\partial a}{\\partial a} + \\frac{\\partial b}{\\partial a}  && node3 \\Leftarrow node1 + node2 \\\\\n",
    "d=b+1 && \\frac{\\partial d}{\\partial a} = \\frac{\\partial b}{\\partial a} + \\frac{\\partial 1}{\\partial a} && node5 \\Leftarrow node2 + node4  \\\\\n",
    "e=c*d && \\frac{\\partial e}{\\partial a} = c*\\frac{\\partial d}{\\partial a} + \\frac{\\partial c}{\\partial a}*d && node6 \\Leftarrow node3*node5 \\\\\n",
    "\\end{array}\n",
    "\n",
    "To simulate the above steps, we have a type ADFwd which also represents a node in the calculation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type to store the float value for a variable and\n",
    "# the derivative with repect to the variable at that value.\n",
    "type ADFwd\n",
    "    value::Float64 # say, to store c\n",
    "    derivative::Float64 # say, to store dc/da\n",
    "    \n",
    "    ADFwd(val::Float64) = new(val, 0)\n",
    "    ADFwd(val::Float64, der::Float64) = new(val, der)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the operation on this type, and also the derivation rule to follow. Operator overloading helps here in the operations over the type ADFwd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "* (generic function with 150 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum rule\n",
    "function adf_add(x::ADFwd, y::ADFwd)\n",
    "    return ADFwd(x.value + y.value, x.derivative + y.derivative)\n",
    "end\n",
    "+(x::ADFwd, y::ADFwd) = adf_add(x, y)\n",
    "\n",
    "# product rule\n",
    "function adf_mul(x::ADFwd, y::ADFwd)\n",
    "    return ADFwd(x.value * y.value, y.value * x.derivative + x.value * y.derivative)\n",
    "end\n",
    "*(x::ADFwd, y::ADFwd) = adf_mul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define test function\n",
    "function f(x::ADFwd,y::ADFwd)\n",
    "    (x+y)*(y + ADFwd(1.0))\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get the partial derivative of the above function with respect to 'a'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADFwd(1.0,0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define variables\n",
    "aFwd = ADFwd(2.0, 1.0)\n",
    "bFwd = ADFwd(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward mode AD\n",
    "eaFwd = f(aFwd, bFwd)\n",
    "eaFwd.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculated derivative: de/da\n",
    "eaFwd.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for 'b'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADFwd(1.0,1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define variables\n",
    "aFwd = ADFwd(2.0)\n",
    "bFwd = ADFwd(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward mode AD\n",
    "ebFwd = f(aFwd, bFwd)\n",
    "ebFwd.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculated derivative: de/db\n",
    "ebFwd.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative result will be present in the output ADFwd type variable. It represents the change in the output dependent variable with respect to the change in the input independent variable. The forward mode is simple to implement and does not take much memory. But if we have to calculate the derivative with respect to multiple variables then we need to do the  forward pass for each variable. In such cases, reverse mode AD proves useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"section-heading\">Reverse Mode</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reverse mode helps in understanding the change in the inputs with respect to the change in the output. The first half of the reverse made is similar to the calculations as in the forward mode, we just don't calculate the derivatives. We move forward in the graph calculating the actual value of the sub-expression and then on reaching the output node, we set the output dependent variable's derivative component as 1. We use this derivative component along with the actual values calculated in the forward pass to apply the chain rule and calculate the derivative components for the parent dependent variable(s) and so on until the independent variables are reached. \n",
    "\n",
    ">In a pen-and-paper calculation, one can perform the equivalent by repeatedly substituting the derivative of the outer functions in the chain rule:\n",
    ">\n",
    "$${\\displaystyle {\\frac {\\partial y}{\\partial x}}={\\frac {\\partial y}{\\partial w_{1}}}{\\frac {\\partial w_{1}}{\\partial x}}=\\left({\\frac {\\partial y}{\\partial w_{2}}}{\\frac {\\partial w_{2}}{\\partial w_{1}}}\\right){\\frac {\\partial w_{1}}{\\partial x}}=\\left(\\left({\\frac {\\partial y}{\\partial w_{3}}}{\\frac {\\partial w_{3}}{\\partial w_{2}}}\\right){\\frac {\\partial w_{2}}{\\partial w_{1}}}\\right){\\frac {\\partial w_{1}}{\\partial x}}=\\cdots }$$\n",
    "><p>- <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\">wikipedia</a></p>\n",
    "\n",
    "We can see the equations during the reverse pass as:\n",
    "\n",
    "\\begin{array}{cc}\n",
    "\\mathrm{derivative} && child node \\Leftarrow parent node\\\\\n",
    "\\hline \\\\\n",
    "\\frac{\\partial e}{\\partial e} = 1  && node6\\\\\n",
    "\\frac{\\partial e}{\\partial c} = \\frac{\\partial e}{\\partial e}*\\frac{\\partial e}{\\partial c} = 1*d && node 3 \\Leftarrow node 6\\\\\n",
    "\\frac{\\partial e}{\\partial d} = \\frac{\\partial e}{\\partial e}*\\frac{\\partial e}{\\partial d} = 1*c && node5 \\Leftarrow node6 \\\\\n",
    "\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c}*\\frac{\\partial c}{\\partial a} = d*1 && node1 \\Leftarrow node3 \\\\\n",
    "\\frac{\\partial e}{\\partial b} = \\frac{\\partial e}{\\partial c}*\\frac{\\partial c}{\\partial b} + \\frac{\\partial e}{\\partial d}*\\frac{\\partial d}{\\partial b} =  d*1 + c*1 && node2 \\Leftarrow node3,node5 \\\\\n",
    "\\end{array}\n",
    "\n",
    "In the implementation, we have a type ADRev which stores the value and the derivative for a particular node. We also store the parents during the forward pass to propagate the derivative backwards during the reverse pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ad_constD (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type to store the float value for a variable during the forward pass\n",
    "# and the derivative during the reverse pass. \n",
    "type ADRev\n",
    "    value::Float64\n",
    "    derivative::Float64\n",
    "    derivativeOp::Function\n",
    "    parents::Array{ADRev}\n",
    "    \n",
    "    ADRev(val::Float64) = new(val, 0, ad_constD, Array(ADRev,0))\n",
    "    ADRev(val::Float64, der::Float64) = new(val, der, ad_constD, Array(ADRev,0))\n",
    "end\n",
    "\n",
    "function ad_constD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+ (generic function with 165 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the actual addition operation and the derivative rule to use\n",
    "# during the reverse pass.\n",
    "function adr_add(x::ADRev, y::ADRev)\n",
    "    result = ADRev(x.value + y.value)\n",
    "    result.derivativeOp = adr_addD\n",
    "    push!(result.parents, x)\n",
    "    push!(result.parents, y)\n",
    "    return result\n",
    "end\n",
    "function adr_addD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    adNodes[1].derivative = adNodes[1].derivative + prevDerivative * 1\n",
    "    adNodes[2].derivative = adNodes[2].derivative + prevDerivative * 1\n",
    "    return\n",
    "end\n",
    "+(x::ADRev, y::ADRev) = adr_add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "* (generic function with 151 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the actual multiplication operation and the derivative rule to use\n",
    "# during the reverse pass.\n",
    "function adr_mul(x::ADRev, y::ADRev)\n",
    "    result = ADRev(x.value * y.value)\n",
    "    result.derivativeOp = adr_mulD\n",
    "    push!(result.parents, x)\n",
    "    push!(result.parents, y)\n",
    "    return result\n",
    "end\n",
    "function adr_mulD(prevDerivative::Float64, adNodes::Array{ADRev})\n",
    "    adNodes[1].derivative = adNodes[1].derivative + prevDerivative * adNodes[2].value\n",
    "    adNodes[2].derivative = adNodes[2].derivative + prevDerivative * adNodes[1].value\n",
    "    return\n",
    "end\n",
    "*(x::ADRev, y::ADRev) = adr_mul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing a breadth-first graph traversal to propagate the derivatives backward during the reverse pass. Since the objects are passed using reference, updating the parent having multiple children becomes trivial. For example, node 2 needs to accumulate the derivate from node 3 and node 5 in our case, both of which may get evaluated separately during the traversal. And this is why we are adding the calculated derivative instead of directly assigning it to the node's derivative.\n",
    "\n",
    "```Julia\n",
    "adNodes[1].derivative = adNodes[1].derivative + ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chainRule (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the reverse pass where we apply the chain rule\n",
    "function chainRule(graph::ADRev)\n",
    "    current = graph\n",
    "    # set the derivative to 1\n",
    "    current.derivative = 1\n",
    "    bfs = [current]\n",
    "    while length(bfs) != 0\n",
    "        current = pop!(bfs)\n",
    "        currDerivative = current.derivative\n",
    "        current.derivativeOp(currDerivative, current.parents)\n",
    "        numParents = length(current.parents)\n",
    "        for i=1:numParents \n",
    "            push!(bfs, current.parents[i])\n",
    "        end\n",
    "    end\n",
    "    return graph\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function\n",
    "function f(x::ADRev,y::ADRev)\n",
    "    (x+y)*(y + ADRev(1.0))\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADRev(1.0,0.0,ad_constD,ADRev[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the variables\n",
    "aRev = ADRev(2.0)\n",
    "bRev = ADRev(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADRev(6.0,0.0,adr_mulD,ADRev[ADRev(3.0,0.0,adr_addD,ADRev[ADRev(2.0,0.0,ad_constD,ADRev[]),ADRev(1.0,0.0,ad_constD,ADRev[])]),ADRev(2.0,0.0,adr_addD,ADRev[ADRev(1.0,0.0,ad_constD,ADRev[]),ADRev(1.0,0.0,ad_constD,ADRev[])])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "eRev_forward = f(aRev, bRev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADRev(6.0,1.0,adr_mulD,ADRev[ADRev(3.0,2.0,adr_addD,ADRev[ADRev(2.0,2.0,ad_constD,ADRev[]),ADRev(1.0,5.0,ad_constD,ADRev[])]),ADRev(2.0,3.0,adr_addD,ADRev[ADRev(1.0,5.0,ad_constD,ADRev[]),ADRev(1.0,3.0,ad_constD,ADRev[])])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse pass\n",
    "eRev_reverse = chainRule(eRev_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are storing the graph during the forward pass to help us in the reverse pass, the output variable can explain the parent-child relationship as well as the operations performed on each of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derivative with respect to all the independent variables\n",
    "aRev.derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bRev.derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the benefit of using reverse mode AD is that we can calculate the derivative of the output with respect to each of the input variables in a single iteration only. We'll use this property to implement a neural network in the coming post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2 class=\"section-heading\">References:</h2>\n",
    "\n",
    "- [Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "- [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [ALGORITHMIC/AUTOMATIC DIFFERENTIATION](http://blog.tombowles.me.uk/2014/09/10/ad-algorithmicautomatic-differentiation/)\n",
    "- [Gradients via Reverse Accumulation](http://www.win-vector.com/dfiles/ReverseAccumulation.pdf)\n",
    "- [Step-by-step example of reverse-mode automatic differentiation](http://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation)\n",
    "- [Efficient Calculation of Derivatives using Automatic Differentiation](https://www.duo.uio.no/bitstream/handle/10852/41535/Kjelseth-Master.pdf?sequence=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
