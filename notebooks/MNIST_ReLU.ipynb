{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pkg.add(\"MNIST\");\n",
    "#Pkg.add(\"PyPlot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MNIST\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "# ===================\n",
    "# load training data\n",
    "# ===================\n",
    "X,y = traindata(); #X:(784x60000), y:(60000x1)\n",
    "X /= 255.0; # scale the input between 0 and 1\n",
    "X = X'; #X:(60000X784)\n",
    "y = y+1; # adding 1 to handle index, hence value 1 represent digit 0 now\n",
    "# number of instances\n",
    "println(size(y,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const inputLayerSize = size(X,2);\n",
    "const hiddenLayerSize = 100;\n",
    "const outputLayerSize = 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a network\n",
    "network = [];\n",
    "numIterations = 1000;\n",
    "const alpha = 1e-0; # step size\n",
    "const lambda = 1e-3; # regularization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add first layer to the network\n",
    "layer1 = Array(Matrix{Float64}, 1,2)\n",
    "#theta1\n",
    "layer1[1] = 0.01*randn(inputLayerSize, hiddenLayerSize); \n",
    "#bias1\n",
    "layer1[2] = zeros(1, hiddenLayerSize); \n",
    "push!(network,layer1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add second layer to the network\n",
    "layer2 = Array(Matrix{Float64}, 1,2)\n",
    "#theta2\n",
    "layer2[1] = 0.01*randn(hiddenLayerSize, outputLayerSize); \n",
    "#bias2\n",
    "layer2[2] = zeros(1, outputLayerSize); \n",
    "push!(network,layer2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forwardNN (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forwardNN(x::Matrix{Float64})\n",
    "    global network;\n",
    "    # collect input for each layer\n",
    "    activation = Matrix{Float64}[];\n",
    "    # initialize input vector with the actual data\n",
    "    push!(activation, x);\n",
    "    for layer in 1:length(network)-1\n",
    "        push!(activation, max(0.0, (activation[layer]*network[layer][1]) .+ network[layer][2]))\n",
    "    end\n",
    "    # compute the class probabilities\n",
    "    # softmax on last layer\n",
    "    score = activation[length(network)]*network[length(network)][1] .+ network[length(network)][2]\n",
    "    exp_scores = exp(score);\n",
    "    probs = exp_scores ./ sum(exp_scores, 2); # [N x K]\n",
    "\n",
    "    return activation,probs;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backwardNN (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backwardNN(a::Vector{Matrix{Float64}}, y::Vector{Float64}, dscores::Matrix{Float64})\n",
    "    global network;\n",
    "    m = size(y,1);\n",
    "    delta = Array(Matrix{Float64}, 1,length(network));\n",
    "    # start from the last layer to backpropagate the error\n",
    "    # compute the gradient on scores\n",
    "    for j in 1:size(dscores,1)\n",
    "        dscores[j,convert(Int32, y[j])] -= 1;\n",
    "    end\n",
    "    delta[length(network)] = dscores / m;\n",
    "    \n",
    "    for j in length(network)-1:-1:1\n",
    "        # backpropate the gradient to the parameters\n",
    "        dhidden = delta[j+1]*network[j+1][1]';\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[a[j+1] .<= 0] = 0;\n",
    "        delta[j] = dhidden;\n",
    "    end\n",
    "    return delta;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateThetas (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateThetas(a::Vector{Matrix{Float64}}, delta::Matrix{Matrix{Float64}})\n",
    "    global network;\n",
    "    for j in 1:length(network)\n",
    "        # update theta\n",
    "        network[j][1] =  network[j][1] - alpha*(a[j]'*delta[j] + lambda*network[j][1])\n",
    "        # update bias\n",
    "        network[j][2] =  network[j][2] - alpha*sum(delta[j],1);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [cross-entropy should be used to measure the distance of two distributions, not any vectors. So, if there is zero in your vector, cross-entropy doesn't make sense. It's better to use Euclidean distance.](https://groups.google.com/forum/#!topic/theano-users/tn0ang57mfE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "costFunction (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function costFunction(truth::Vector{Float64}, probability::Matrix{Float64})\n",
    "    global network;\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    m = size(truth,1)\n",
    "    \n",
    "    corect_logprobs = [-log(probability[j,convert(Int32, y[j])]) for j in 1:m];\n",
    "    data_loss = sum(corect_logprobs)/m;\n",
    "    \n",
    "    reg_loss = 0;\n",
    "    for j in 1:length(network)\n",
    "        reg_loss = reg_loss + 0.5*lambda*sum(network[j][1].^2);\n",
    "    end\n",
    "    \n",
    "    loss = data_loss + reg_loss;\n",
    "    return loss;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: [0.3425860661080752]\n",
      "cost: [0.27579192159873106]\n",
      "cost: [0.2272569768576536]\n",
      "cost: [0.20965762253801098]\n",
      "cost: [0.20493261136365065]\n",
      "cost: [0.19439090346226265]\n",
      "cost: [0.2789278047077673]\n",
      "cost: [0.24548923798270034]\n",
      "cost: [0.22795710495999277]\n",
      "cost: [0.20856739145022474]\n"
     ]
    }
   ],
   "source": [
    "J = zeros(numIterations,1);\n",
    "for itr in 1:numIterations\n",
    "    # feedforwarf\n",
    "    activations, probs = forwardNN(X); \n",
    "    # cost \n",
    "    if itr%100 ==0\n",
    "        J[itr,:] = costFunction(y, probs);\n",
    "        println(\"cost: \", J[itr,:]);\n",
    "    end\n",
    "    # backpropagation\n",
    "    newThetas = backwardNN(activations, y, probs);\n",
    "    # update parameters\n",
    "    updateThetas(activations, newThetas);\n",
    "end\n",
    "# print network to weight_file\n",
    "# print ids to id_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(data::Matrix{Float64})\n",
    "    activation, probs = forwardNN(data);\n",
    "    predicted_class = [indmax(probs[j,:]) for j in 1:size(probs,1)]\n",
    "    return predicted_class;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy(truth, prediction)\n",
    "    correct = 0;\n",
    "    for j in 1:length(truth)\n",
    "        if truth[j] == prediction[j]\n",
    "            correct = correct + 1;\n",
    "        end\n",
    "    end\n",
    "    println(\"training accuracy: \", correct/length(truth)*100);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 96.86833333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.matshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "XTest,yTest = testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XTest /= 255.0;\n",
    "XTest = XTest';\n",
    "yTest = yTest+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predTest = predict(XTest);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 96.19\n"
     ]
    }
   ],
   "source": [
    "accuracy(yTest, predTest);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
