{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/JuliaLang/julia/issues/14099\n",
    "const spaces = filter(isspace, Char(0):Char(0x10FFFF));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readWordTagData (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readWordTagData(filePath)\n",
    "    file = open(filePath);\n",
    "    vocabSet = Set{AbstractString}();\n",
    "    tagSet = Set{AbstractString}();\n",
    "    # read line\n",
    "    for ln in eachline(file)\n",
    "        word_tag = split(ln, spaces);\n",
    "        # remove \"\"\n",
    "        word_tag = word_tag[word_tag .!= \"\"]\n",
    "        # separate word from tag\n",
    "        for token in word_tag\n",
    "            tokenSplit = split(token, \"_\");\n",
    "            push!(vocabSet, tokenSplit[1]);\n",
    "            push!(tagSet, tokenSplit[2]);\n",
    "        end\n",
    "    end\n",
    "    close(file);\n",
    "    # to handle unknown words\n",
    "    push!(vocabSet, \"UNK_W\");\n",
    "    # to handle unknown tags\n",
    "    push!(tagSet, \"UNK_T\");\n",
    "    #println(vocabSet)\n",
    "    #println(tagSet)\n",
    "    vocab = collect(vocabSet);\n",
    "    tags = collect(tagSet);\n",
    "    # prepare data array\n",
    "    data = Tuple{AbstractString , AbstractString }[];\n",
    "    file = open(filePath);\n",
    "    # read line\n",
    "    for ln in eachline(file)\n",
    "        word_tag = split(ln, spaces);\n",
    "        # remove \"\"\n",
    "        word_tag = word_tag[word_tag .!= \"\"]\n",
    "        # separate word from tag\n",
    "        for token in word_tag\n",
    "            tokenSplit = split(token, \"_\");\n",
    "            push!(data, (tokenSplit[1], tokenSplit[2]));\n",
    "        end\n",
    "    end\n",
    "    close(file);\n",
    "    #println(length(data))\n",
    "    return vocab, tags, data;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tanhGradient (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tanhGradient(x)\n",
    "    return (1 - (x.*x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1x4 Array{Float64,2}:\n",
       " -1.52663  0.57886  1.27963  0.44816"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "vocabListTrain, tagListTrain, dataTrain = readWordTagData(\"data/pos/05-train-input.txt\");\n",
    "#readWordTagData(\"data/pos/wiki-en-train.norm_pos\");\n",
    "#\n",
    "\n",
    "# define the network\n",
    "inputLayerSize = length(vocabListTrain);\n",
    "hiddenLayerSize = 100;\n",
    "outputLayerSize = length(tagListTrain);\n",
    "learningRate = 1e-1;\n",
    "# initialize weights and biases\n",
    "Wxh = randn(inputLayerSize, hiddenLayerSize)*0.01; # input to hidden\n",
    "Whh = randn(hiddenLayerSize, hiddenLayerSize)*0.01; # hidden to hidden\n",
    "Bh = randn((1, hiddenLayerSize)); # hidden bias\n",
    "Why = randn(hiddenLayerSize, outputLayerSize)*0.01; # hidden to output\n",
    "By = randn((1, outputLayerSize)); # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- smooth cost\n",
    "- h previous\n",
    "- unfolding rnn with sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradCheck (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient checking\n",
    "function gradCheck(inputs::Array{Vector{Float64},1}, targets::Array{Vector{Float64},1},\n",
    "    h, p::Array{Vector{Float64},1})\n",
    "    hCopyOriginal = deepcopy(h);    \n",
    "    paramNameList = [\"Wxh\", \"Whh\", \"Why\", \"Bh\", \"By\"];\n",
    "    # collect paramters\n",
    "    global Wxh, Whh, Why, bh, by;\n",
    "    paramList = [x for x=(Wxh, Whh, Why, Bh, By)];\n",
    "    num_checks = 2;\n",
    "    delta = 1e-5;\n",
    "    # collect parameter gradients\n",
    "    cost = forwardRNN(inputs, targets, h, p);\n",
    "    dWxh, dWhh, dBh, dWhy, dBy = backwardRNN(inputs, targets, h, p);\n",
    "    dParamList = [x for x=(dWxh, dWhh, dWhy, dBh, dBy)];\n",
    "    for (param,dparam,name) in zip(paramList, dParamList, paramNameList)\n",
    "        # validate the size of the parameter and its gradient\n",
    "        s0 = size(dparam);\n",
    "        s1 = size(param);\n",
    "        if s0 != s1\n",
    "            error(\"Error dims dont match: \", s0,\" and \",s1);\n",
    "        end\n",
    "        println(name)\n",
    "        println(dparam)\n",
    "        for i in 1:num_checks\n",
    "            ri = rand(1:length(param));\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            old_val = param[ri];\n",
    "            param[ri] = old_val + delta;\n",
    "            cg0 = forwardRNN(inputs, targets, hCopyOriginal, p);\n",
    "            param[ri] = old_val - delta;\n",
    "            cg1 = forwardRNN(inputs, targets, hCopyOriginal, p);\n",
    "            param[ri] = old_val # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = dparam[ri];\n",
    "            grad_numerical = (cg0 - cg1) / ( 2 * delta );\n",
    "            \n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic);\n",
    "            println(grad_numerical,\", \", grad_analytic, \" => \",rel_error);\n",
    "            # rel_error should be on order of 1e-7 or less\n",
    "            if rel_error > 1e-5\n",
    "                error(\"Gradient check failed.\");\n",
    "            end\n",
    "            println(\"Gradient check passed.\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forwardRNN (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forwardRNN(x::Array{Vector{Float64},1}, y::Array{Vector{Float64},1}, h, p::Array{Vector{Float64},1})\n",
    "    global Wxh, Whh, Why, bh, by;\n",
    "    cost = 0;\n",
    "    # for each time t in x\n",
    "    # unrolling RNN -> Feedforward NN step\n",
    "    for time in 1:size(x,1)\n",
    "        h[time] = tanh(x[time]' * Wxh + h[time]*Whh .+ Bh);\n",
    "        # output layer\n",
    "        score = h[time]*Why .+ By;\n",
    "        p_softmax = exp(score) / sum(exp(score));\n",
    "        p[time] = vec(p_softmax); # output probability distribution (at time t)\n",
    "        cost += -sum(log(y[time]'*p[time])) # assuming y is a one-hot vector\n",
    "    end\n",
    "    return cost;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backwardRNN (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backwardRNN(x::Array{Vector{Float64},1}, y::Array{Vector{Float64},1}, \n",
    "    h, p::Array{Vector{Float64},1})\n",
    "    \n",
    "    global Wxh, Whh, Why, bh, by;\n",
    "    \n",
    "    dWxh = zeros(size(Wxh));\n",
    "    dWhh = zeros(size(Whh));\n",
    "    dBh = zeros(size(Bh));\n",
    "    dWhy = zeros(size(Why));\n",
    "    dBy = zeros(size(By));\n",
    "    \n",
    "    dh = zeros(size(Bh)); # error from the following time step\n",
    "    for time in size(x,1):-1:1\n",
    "        # output layer error\n",
    "        dy = p[time] - y[time]; #assuming y is a one hot vector\n",
    "        # output gradient\n",
    "        dWhy = dWhy + (dy * h[time])'; \n",
    "        dBy = dBy + dy';\n",
    "        # backpropagate\n",
    "        dh = (dh * Whh)+ (Why*dy)'.* tanhGradient(h[time]) ;\n",
    "        # hidden layer gradient\n",
    "        dWxh = dWxh + (x[time] * dh);\n",
    "        dBh = dBh + dh;\n",
    "        dWhh = dWhh + (h[time]' * dh);\n",
    "    end\n",
    "    # clip to mitigate exploding gradients\n",
    "    dWxh = clamp(dWxh, -5, 5);\n",
    "    dWhh = clamp(dWhh, -5, 5);\n",
    "    dBh = clamp(dBh, -5, 5);\n",
    "    dWhy = clamp(dWhy, -5, 5);\n",
    "    dBy = clamp(dBy, -5, 5);\n",
    "    \n",
    "    return dWxh, dWhh, dBh, dWhy, dBy;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function train(data::Array{Tuple{AbstractString,AbstractString},1}, vocabList::Array{AbstractString,1} \n",
    "    , tagList::Array{AbstractString,1}, numItr::Int64, seqLength::Int64, sampleCostAtItr::Int64)\n",
    "    \n",
    "    global Wxh, Whh, Why, Bh, By;\n",
    "    numIterations =  numItr * length(data);\n",
    "    costList = []; # store cost per sampled iteration\n",
    "    ptr = 1;\n",
    "    p = [zeros(length(tagList)) for i in 1:seqLength];\n",
    "    h = [zeros(1,hiddenLayerSize) for i in 1:seqLength]; # hidden layers (at time t)\n",
    "   for itr in 1:numIterations\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if ptr+seqLength-1 > length(data) # whenever we are looking at the data from the start\n",
    "            # reset RNN memory\n",
    "            h[1] = zeros(1, hiddenLayerSize);\n",
    "            # go from start of data\n",
    "            ptr = 1 \n",
    "        end\n",
    "        # generate sequence\n",
    "        seqData = data[ptr:ptr+seqLength-1];\n",
    "        x = Vector{Float64}[];\n",
    "        y = Vector{Float64}[];\n",
    "        for word_tag in seqData\n",
    "            word = word_tag[1];\n",
    "            tag = word_tag[2];\n",
    "            # convert to one-hot vectors\n",
    "            # words\n",
    "            wordVec = zeros(length(vocabList));\n",
    "            findWord = collect(vocabList.==word)\n",
    "            if length(findWord[findWord.==true]) == 0\n",
    "                # unknown word: UNK_W\n",
    "                findWord[length(findWord)] = true;\n",
    "            end\n",
    "            wordVec[findWord] = 1;\n",
    "            # tags\n",
    "            tagVec = zeros(length(tagList));\n",
    "            findTag = collect(tagList.==tag)\n",
    "            if length(findTag[findTag.==true]) == 0\n",
    "                # unknown tag: UNK_T\n",
    "                findTag[length(findTag)] = true;\n",
    "            end\n",
    "            tagVec[findTag] = 1;\n",
    "            # push to the sequence\n",
    "            push!(x , wordVec);\n",
    "            push!(y , tagVec);\n",
    "        end\n",
    "        # gradient check\n",
    "        gradCheck(x, y, h, p);\n",
    "    \n",
    "        \n",
    "    end\n",
    "    return costList;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxh"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: Gradient check failed.\nwhile loading In[38], in expression starting on line 8",
     "output_type": "error",
     "traceback": [
      "LoadError: Gradient check failed.\nwhile loading In[38], in expression starting on line 8",
      "",
      " in gradCheck at In[34]:41",
      " in train at In[37]:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " -0.007019477216706601 -0.0015302056039462615 0.0008898375890206236 -7.982268867565617e-5 0.010682885569259922 -0.004058140767092821 0.00023501836237258664 0.002617057293443229 0.004545468042978159 0.004794149027872132 0.0025899910425947704 0.0011441875301506014 0.0035503195784109217 -0.0027544452895073806 0.007997459253840036 0.007554769327718688 -0.00025364906663728493 0.008231711426996274 -0.005482536060322134 -0.003299030890410024 0.007484233610399052 -0.004387085115996555 0.0011164530531272588 0.0005334927430210279 -0.0021805342596975773 0.016004074940090745 0.003654789092936577 0.00690712554561203 -0.0005281419617146523 -0.00526884997060082 -0.019301012744669825 0.0006265915511899062 -0.00890468696107006 0.006024932597717353 0.005376499823720496 -0.009470641463272905 0.0032387137801384554 0.005008662687867647 -0.004795510058414809 -0.011914958346383807 -0.0036645685938433478 -0.007184034279199269 0.006494787907351403 -0.002556994490520438 -0.0032074349669355975 -0.018926624401936626 0.00941275974075717 -0.006268578231420854 -0.009949512471610916 0.008065810122105945 0.0063205313317151865 -0.001838170622516881 -0.012306340371813721 -0.006209455229841654 0.0009203778109787766 0.003461051186361531 -0.009738995991866808 0.016388922174260002 0.001964689789198838 -0.00010569686435991837 -0.0010846265665177307 0.0019178295833210293 -0.0013696504643337408 0.003750293332744894 0.0006601766539930459 0.0029789653180775353 -0.009946732699572192 -0.007924969018058345 -0.011575109723208908 0.00542206310576311 -0.0006339027039725777 0.01110575126636481 0.00018111288838658284 -0.002999061159044712 -0.008972065980758224 -0.017578958683397344 -0.00028333325370225254 -0.004774036552865916 -2.7485904941876234e-5 -0.0022371670662546844 -0.0038527066463719633 -0.007351786807497584 -0.001084355528169132 0.009492808762699888 -0.013880048265724717 0.00864159787602544 -4.614351505882868e-5 -0.0005605898210631384 0.008925331641940955 0.01189370834079312 -0.011601067213956118 0.01736976896796737 -0.023402870953046835 0.013863142846207347 0.012658095404102643 -0.00032793102927819554 0.0009843398599626004 0.012493048325952985 -0.0010333318111175743 0.005389980586101523\n",
      " 0.0022520550923201483 0.005416875464155494 -0.0012409177645053085 4.702557915957567e-5 -0.0008254832202515705 -0.004466739750923932 -7.89065754183998e-5 0.0011379107789939481 0.0014119271714065713 -0.0028072982727276197 -0.0014555839162006463 -0.004054097443015192 -0.0010931994743502865 0.003780101098900436 -0.009793445383152083 -0.003536791722166286 -0.00024113033329509637 -0.0001600842915191483 -0.0031540394614564884 -0.004230479455044679 -0.005138937126134841 -0.0012682063868882378 -2.6382399126780596e-6 -0.004391946417490291 -0.0011583138604543972 -0.006221831715821608 -0.0037043241142483397 -0.003135809866912844 -0.0026237895810096975 0.0005415227086008685 0.013405579308449826 -0.002351732808753406 0.0026599734531500465 -0.00269157144678687 0.0004544458510259369 0.0025983398101166078 -0.0006335433133432345 -0.003204197505674293 0.005540943771369293 -0.002178004191387776 0.0086956125588227 0.004590293579848683 0.007165808054036282 0.006142783789709206 -0.0004935811180111427 -0.00012387458725353042 -0.0002969746194628391 -0.0007088672580450674 0.0056192497294534835 0.0008882685083476157 -0.0051737085740917454 -0.0007335521058674655 0.0032488448537255315 -0.003387395270902994 0.0017565432301018705 -0.009561807983486548 0.0003868145911948148 -0.0007126814984723597 0.0013337758309582195 0.00042185676673249967 -0.005219754073313473 -0.006222300101135429 -0.004246176377502618 0.007375791716193681 -0.003610709777890103 -0.002056736466375713 -0.0027549991441777237 -0.00011368804270958953 -0.006269269667165042 -0.0055746111174083774 0.000604589427500722 -0.015316764128814843 -0.0013444993838244562 -0.0003391235426470318 -0.0013126444314557138 -0.005141473692511182 -7.577121968368235e-5 -0.002665575442956208 0.0017147673032817307 0.007044300294935248 0.004315648621561997 0.006696814783882252 0.0034356172819945646 -0.0057231432951605895 -0.0008880006758475729 0.00013515656940997286 -0.0004159581459141224 0.0008841753040334305 -0.0030717737934471616 -0.004458322424763164 -0.0038782598795586956 -0.0055783454349614126 0.007513889244874382 0.006939500788231562 -0.008330887450873703 0.00013847566826678064 0.0004432665052575746 -0.0042148988108614135 -0.0006287493237770413 0.002702953766488823]\n",
      "-112.3106285789932, 0.0 => 1.0\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "# number of steps to unroll the RNN for\n",
    "seqLen = 2 \n",
    "# run through the data n times\n",
    "numIterOverData = 1;\n",
    "# sample cost after each n iteration\n",
    "sampleCostAtEveryItr = 1;\n",
    "J = train(dataTrain, vocabListTrain, tagListTrain, numIterOverData, seqLen, sampleCostAtEveryItr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cost per iteration\n",
    "sampleIdxJ = [1+sampleCostAtEveryItr*i for i in 0:length(J)-1]\n",
    "plot(sampleIdxJ, J)\n",
    "xlabel(\"Sampled Iterations\")\n",
    "ylabel(\"Cost\")\n",
    "grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function findAccuracy(data::Array{Tuple{AbstractString,AbstractString},1}, vocabList::Array{AbstractString,1} \n",
    "    , tagList::Array{AbstractString,1}, seqLength::Int64)\n",
    "    \n",
    "    correct = 0;\n",
    "    ptr=1;\n",
    "    prob = [zeros(length(tagList)) for i in 1:seqLength];\n",
    "    h = [zeros(1,hiddenLayerSize) for i in 1: seqLength]; # hidden layers (at time t)\n",
    "    for i in 1:length(data)/seqLength\n",
    "        \n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if ptr+seqLength-1 > length(data) # whenever we are looking at the data from the start\n",
    "            break # return if data is alread read\n",
    "        end\n",
    "        # generate sequence\n",
    "        seqData = data[ptr:ptr+seqLength-1];\n",
    "        x = Vector{Float64}[];\n",
    "        y = Vector{Float64}[];\n",
    "        for word_tag in seqData\n",
    "            word = word_tag[1];\n",
    "            tag = word_tag[2];\n",
    "            # convert to one-hot vectors\n",
    "            # words\n",
    "            wordVec = zeros(length(vocabList));\n",
    "            findWord = collect(vocabList.==word)\n",
    "            if length(findWord[findWord.==true]) == 0\n",
    "                # unknown word: UNK_W\n",
    "                findWord[length(findWord)] = true;\n",
    "            end\n",
    "            wordVec[findWord] = 1;\n",
    "            # tags\n",
    "            tagVec = zeros(length(tagList));\n",
    "            findTag = collect(tagList.==tag)\n",
    "            if length(findTag[findTag.==true]) == 0\n",
    "                # unknown tag: UNK_T\n",
    "                findTag[length(findTag)] = true;\n",
    "            end\n",
    "            tagVec[findTag] = 1;\n",
    "            # push to the sequence\n",
    "            push!(x , wordVec);\n",
    "            push!(y , tagVec);\n",
    "        end\n",
    "        # feedforward\n",
    "        for time in 1:size(seqData,1)\n",
    "            h[time] = tanh(x[time]' * Wxh + h[time]*Whh .+ Bh);\n",
    "            # output layer\n",
    "            score = h[time]*Why .+ By;\n",
    "            p_softmax = exp(score) / sum(exp(score));\n",
    "            prob[time] = vec(p_softmax); # output probability distribution (at time t)\n",
    "        end\n",
    "        h[1] = h[size(h,1)];\n",
    "        \n",
    "        prediction = [indmax(prob[j]) for j in 1:size(seqData,1)];\n",
    "        truth = [indmax(y[j]) for j in 1:size(seqData,1)];\n",
    "        # accuracy\n",
    "        for j in 1:length(seqData)\n",
    "            if truth[j] == prediction[j]\n",
    "                correct = correct + 1;\n",
    "            end\n",
    "        end\n",
    "        ptr += seqLength; # move data pointer\n",
    "    end\n",
    "    accuracy = correct/length(data)*100;\n",
    "    return accuracy;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = findAccuracy(dataTrain, vocabListTrain, tagListTrain, seqLen);\n",
    "println(\"accuracy: \", accuracy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "vocabListTest, tagListTest, dataTest  = readWordTagData(\"data/pos/05-test-input.txt\");\n",
    "#readData(\"data/pos/wiki-en-test.norm\");\n",
    "# will use vocab list and tag list created at the training time\n",
    "#accuracy, result = findAccuracy(dataTest, vocabListTrain, tagListTrain, seqLen);\n",
    "println(\"accuracy: \", accuracy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://youtu.be/56TYLaQN4N8)\n",
    "- [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](http://arxiv.org/abs/1412.6980)\n",
    "- [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://jmlr.org/papers/v12/duchi11a.html)\n",
    "- [NLP Programming Tutorial](http://www.phontron.com/slides/nlp-programming-en-08-rnn.pdf)\n",
    "- [Lec [5.1]: Deep Learning, Recurrent neural network](https://youtu.be/AvyhbrQptHk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
