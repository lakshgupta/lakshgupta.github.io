{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/JuliaLang/julia/issues/14099\n",
    "const spaces = filter(isspace, Char(0):Char(0x10FFFF));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open(\"data/pos/05-train-input.txt\");\n",
    "vocabSet = Set();\n",
    "tagSet = Set();\n",
    "# read line\n",
    "for ln in eachline(file)\n",
    "    word_tag = split(ln, spaces);\n",
    "    # remove \"\"\n",
    "    word_tag = word_tag[word_tag .!= \"\"]\n",
    "    # separate word from tag\n",
    "    for token in word_tag\n",
    "        tokenSplit = split(token, \"_\");\n",
    "        push!(vocabSet, tokenSplit[1]);\n",
    "        push!(tagSet, tokenSplit[2]);\n",
    "    end\n",
    "end\n",
    "close(file);\n",
    "#println(vocabSet)\n",
    "#println(tagSet)\n",
    "# vocabulary dict\n",
    "wordDict = Dict{AbstractString, Vector{Float64}}();\n",
    "vocabSize = length(vocabSet);\n",
    "for (index, value) in enumerate(vocabSet)\n",
    "    val = zeros(vocabSize);\n",
    "    val[index] = 1;\n",
    "    wordDict[value] = val;\n",
    "end\n",
    "#println(wordDict);\n",
    "# tag dict\n",
    "tagDict = Dict{AbstractString, Vector{Float64}}();\n",
    "tagSize = length(tagSet);\n",
    "for (index, value) in enumerate(tagSet)\n",
    "    val = zeros(tagSize);\n",
    "    val[index] = 1;\n",
    "    tagDict[value] = val;\n",
    "end\n",
    "#println(tagDict);\n",
    "# prepare data array\n",
    "data = Tuple{AbstractString , AbstractString }[];\n",
    "file = open(\"data/pos/05-train-input.txt\");\n",
    "# read line\n",
    "for ln in eachline(file)\n",
    "    word_tag = split(ln, spaces);\n",
    "    # remove \"\"\n",
    "    word_tag = word_tag[word_tag .!= \"\"]\n",
    "    # separate word from tag\n",
    "    for token in word_tag\n",
    "        tokenSplit = split(token, \"_\");\n",
    "        push!(data, (tokenSplit[1], tokenSplit[2]));\n",
    "    end\n",
    "end\n",
    "close(file);\n",
    "#println(length(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function tanhGradient(x)\n",
    "    return (1 - (x.*x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputLayerSize = length(vocabSet);\n",
    "hiddenLayerSize = 100;\n",
    "outputLayerSize = length(tagSet);\n",
    "learningRate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function forwardRNN(activationFn::Function, x::Array{Array{Float64,1},1}, \n",
    "    y::Array{Array{Float64,1},1}, hPrev::Array{Float64,2})\n",
    "    \n",
    "    h = Matrix{Float64}[]; # hidden layers (at time t)\n",
    "    p = Matrix{Float64}[]; # output probability distribution (at time t)\n",
    "    cost = 0;\n",
    "    # for each time t in x\n",
    "    # unrolling RNN -> Feedforward NN step\n",
    "    for time in 1:length(x)\n",
    "        if time > 1\n",
    "            push!(h, activationFn(Wxh' * x[time] + Whh' * h[time - 1] .+ Bh));\n",
    "        else\n",
    "            push!(h, activationFn(Wxh' * x[time] + Whh' * hPrev .+ Bh));  \n",
    "        end\n",
    "        # output layer\n",
    "        score = Why' * h[time] .+ By;\n",
    "        p_softmax = exp(score) / sum(exp(score));\n",
    "        push!(p, p_softmax);\n",
    "        cost += -sum(log(y[time]'*p[time])) # assuming y is a one-hot vector\n",
    "    end\n",
    "    return h, p, cost\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function backwardRNN(activationFnGrad::Function, x::Array{Array{Float64,1},1},\n",
    "     h::Array{Array{Float64,2},1}, p::Array{Array{Float64,2},1}, y::Array{Array{Float64,1},1})\n",
    "    \n",
    "    dWxh = zeros(size(Wxh));\n",
    "    dWhh = zeros(size(Whh));\n",
    "    dBh = zeros(size(Bh));\n",
    "    dWhy = zeros(size(Why));\n",
    "    dBy = zeros(size(By));\n",
    "    \n",
    "    dh = zeros(length(Bh)); # error from the following time step\n",
    "    for time in 1:-1:length(x)\n",
    "        # output layer error\n",
    "        dy = p[time] - y[time]; #assuming y is a one hot vector\n",
    "        # output gradient\n",
    "        dWhy = dWhy + (h[time] * dy'); \n",
    "        dBy = dBy + dy;\n",
    "        # backpropagate\n",
    "        dh = ((Whh * dh) + (Why * dy)) .* activationFnGrad(h[time]) ;\n",
    "        # hidden layer gradient\n",
    "        dWxh = dWxh + (x[time] * dh');\n",
    "        dBh = dBh + dh;\n",
    "        if time != 1\n",
    "            dWhh = dWhh + (h[time - 1] * dh');\n",
    "        end\n",
    "    end\n",
    "    # clip to mitigate exploding gradients\n",
    "    dWxh = clamp(dWxh, -5, 5);\n",
    "    dWhh = clamp(dWhh, -5, 5);\n",
    "    dBh = clamp(dBh, -5, 5);\n",
    "    dWhy = clamp(dWhy, -5, 5);\n",
    "    dBy = clamp(dBy, -5, 5);\n",
    "    \n",
    "    return dWxh, dWhh, dBh, dWhy, dBy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function updateWeights(dWxh::Array{Float64,2}, dWhh::Array{Float64,2}, dBh::Array{Float64,2}, \n",
    "    dWhy::Array{Float64,2}, dBy::Array{Float64,2})\n",
    "    global Wxh, Whh, Bh, Why, By;\n",
    "    Wxh += -learningRate * dWxh;\n",
    "    Whh += -learningRate * dWhh;\n",
    "    Bh += -learningRate * dBh;\n",
    "    Why += -learningRate * dWhy;\n",
    "    By += -learningRate * dBy;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = randn(inputLayerSize, hiddenLayerSize)*0.01; # input to hidden\n",
    "Whh = randn(hiddenLayerSize, hiddenLayerSize)*0.01; # hidden to hidden\n",
    "Bh = zeros((hiddenLayerSize, 1)); # hidden bias\n",
    "Why = randn(hiddenLayerSize, outputLayerSize)*0.01; # hidden to output\n",
    "By = zeros((outputLayerSize, 1)); # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- smooth cost\n",
    "- h previous\n",
    "- unfolding rnn with sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient checking\n",
    "function gradCheck(inputs, targets, hPrev)\n",
    "    paramNameList = [\"Wxh\", \"Whh\", \"Why\", \"Bh\", \"By\"];\n",
    "    # collect paramters\n",
    "    global Wxh, Whh, Why, bh, by;\n",
    "    paramList = [x for x=(Wxh, Whh, Why, Bh, By)];\n",
    "    num_checks = 10;\n",
    "    delta = 1e-5;\n",
    "    # collect parameter gradients\n",
    "    h, prob, cost = forwardRNN(tanh, inputs, targets, hPrev);\n",
    "    dWxh, dWhh, dBh, dWhy, dBy = backwardRNN(tanhGradient, inputs, h, prob, targets);\n",
    "    dParamList = [x for x=(dWxh, dWhh, dWhy, dBh, dBy)];\n",
    "    # gradient check\n",
    "    for (param,dparam,name) in zip(paramList, dParamList, paramNameList)\n",
    "        # validate the size of the parameter and its gradient\n",
    "        s0 = size(dparam);\n",
    "        s1 = size(param);\n",
    "        if s0 != s1\n",
    "            error(\"Error dims dont match: \", s0,\" and \",s1);\n",
    "        end\n",
    "        #println(name);\n",
    "        for i in 1:num_checks\n",
    "            ri = rand(1,length(param));\n",
    "            println(ri);\n",
    "            #= evaluate cost at [x + delta] and [x - delta]\n",
    "            #old_val = param[:][ri];\n",
    "            #param[:][ri] = old_val + delta;\n",
    "            #h, p, cg0 = forwardRNN(tahn, inputs, targets, hPrev);\n",
    "            #param[:][ri] = old_val - delta;\n",
    "            #h, p, cg1 = forwardRNN(tanh, inputs, targets, hPrev);\n",
    "            #param[:][ri] = old_val # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            #grad_analytic = dparam[:][ri];\n",
    "            #grad_numerical = (cg0 - cg1) / ( 2 * delta );\n",
    "            #rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic);\n",
    "            #println(grad_numerical,\", \", grad_analytic, \" => \",rel_error);\n",
    "            # rel_error should be on order of 1e-7 or less\n",
    "            =#\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqLength = 6 # number of steps to unroll the RNN for\n",
    "# run through the data n times\n",
    "numIterations = 1 #* length(data);\n",
    "ptr = 1; # do not change\n",
    "hPrev = zeros(hiddenLayerSize,1);\n",
    "smoothCost = -log(1.0/vocabSize)*seqLength; # loss at iteration 0\n",
    "J = []; # store cost per iteration\n",
    "# MAIN\n",
    "for itr in 1:numIterations\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if ptr > length(data) || itr == 1 # whenever we are looking at the data from the start\n",
    "        hPrev = zeros(hiddenLayerSize,1) # reset RNN memory\n",
    "        ptr = 1 # go from start of data\n",
    "    end\n",
    "    # prepare the sequence data\n",
    "    seqData = data[ptr:ptr+seqLength-1];\n",
    "    inputs = Vector{Float64}[];\n",
    "    targets = Vector{Float64}[];\n",
    "    for word_tag in seqData\n",
    "        push!(inputs , wordDict[word_tag[1]]);\n",
    "        push!(targets , tagDict[word_tag[2]]);\n",
    "    end\n",
    "    # gradient check\n",
    "    gradCheck(inputs, targets, hPrev);\n",
    "    #=\n",
    "    # feedforward\n",
    "    #h, prob, cost = forwardRNN(tanh, inputs, targets, hPrev);\n",
    "    #smoothCost = smoothCost * 0.999 + cost * 0.001;\n",
    "    #push!(J, smoothCost);\n",
    "    #hPrev = h[length(h)]\n",
    "    # backpropagate\n",
    "    #dWxh, dWhh, dBh, dWhy, dBy = backwardRNN(tanhGradient, inputs, h, prob, targets);\n",
    "    # update weights\n",
    "    #updateWeights(dWxh, dWhh, dBh, dWhy, dBy);\n",
    "    =#\n",
    "    ptr += seqLength # move data pointer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the cost per iteration\n",
    "plot(1:length(J), J)\n",
    "xlabel(\"Iterations\")\n",
    "ylabel(\"Cost\")\n",
    "grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function predict(activationFn, x::Array{Array{Float64,1},1}, y::Array{Array{Float64,1},1})\n",
    "    ptr = 1;\n",
    "    for itr in 1:numIterations\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if ptr > length(x) || itr == 1 # whenever we are looking at the data from the start\n",
    "        hPrev = zeros(hiddenLayerSize,1) # reset RNN memory\n",
    "        ptr = 1 # go from start of data\n",
    "    end\n",
    "    # prepare the sequence data\n",
    "    seqData = data[ptr:ptr+seqLength-1];\n",
    "    inputs = Vector{Float64}[];\n",
    "    targets = Vector{Float64}[];\n",
    "    for word_tag in seqData\n",
    "        push!(inputs , wordDict[word_tag[1]]);\n",
    "        push!(targets , tagDict[word_tag[2]]);\n",
    "    end\n",
    "    # feedforward\n",
    "    h, prob, cost = forwardRNN(tanh, inputs, targets, hPrev);\n",
    "    push!(J, cost);\n",
    "    hPrev = h[length(h)]\n",
    "    # backpropagate\n",
    "    dWxh, dWhh, dBh, dWhy, dBy = backwardRNN(tanhGradient, inputs, h, prob, targets);\n",
    "    # update weights\n",
    "    updateWeights(dWxh, dWhh, dBh, dWhy, dBy);\n",
    "    \n",
    "    ptr += seqLength # move data pointer\n",
    "end\n",
    "        \n",
    "    predicted_class = [indmax(probs[i,:]) for i in 1:size(probs,1)]\n",
    "    return predicted_class;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function accuracy(truth, prediction)\n",
    "    correct = 0;\n",
    "    for i in 1:length(truth)\n",
    "        if truth[i] == prediction[i]\n",
    "            correct = correct + 1;\n",
    "        end\n",
    "    end\n",
    "    println(\"training accuracy: \", correct/length(truth)*100);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://youtu.be/56TYLaQN4N8)\n",
    "- [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](http://arxiv.org/abs/1412.6980)\n",
    "- [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://jmlr.org/papers/v12/duchi11a.html)\n",
    "- [NLP Programming Tutorial](http://www.phontron.com/slides/nlp-programming-en-08-rnn.pdf)\n",
    "- [Lec [5.1]: Deep Learning, Recurrent neural network](https://youtu.be/AvyhbrQptHk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
