{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1 / (1 + np.exp(-x))   \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = (1 - f) * f\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    N = x.shape[0]\n",
    "    x -= np.max(x, axis=1).reshape(N, 1)\n",
    "    x = np.exp(x) / np.sum(np.exp(x), axis=1).reshape(N, 1)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        x[ix] += h \n",
    "        random.setstate(rndstate)\n",
    "        fxph = f(x)[0]\n",
    "        x[ix] -= 2 * h\n",
    "        random.setstate(rndstate)\n",
    "        fxmh = f(x)[0]\n",
    "        x[ix] += h\n",
    "        numgrad = (fxph - fxmh) / (2 * h)   \n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    N, D = data.shape\n",
    "    print N\n",
    "    print D\n",
    "    \n",
    "    print data.dot(W1)\n",
    "    print data.dot(W1)+b1\n",
    "    h = sigmoid(data.dot(W1) + b1)\n",
    "    scores = softmax(h.dot(W2) + b2)\n",
    "    cost = np.sum(- np.log(scores[labels == 1])) / N\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "\n",
    "    dscores = scores - labels\n",
    "    \n",
    "    dscores /= N\n",
    "    \n",
    "    gradb2 = np.sum(dscores, axis=0)\n",
    "    gradW2 = np.dot(h.T, dscores)\n",
    "    \n",
    "    \n",
    "    grad_h = np.dot(dscores, W2.T)\n",
    "    grad_h = sigmoid_grad(h) * grad_h\n",
    "    \n",
    "    gradb1 = np.sum(grad_h, axis=0)\n",
    "    gradW1 = np.dot(data.T, grad_h)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    #grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return #cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "20\n",
      "10\n",
      "[[ 0.32592383  0.91951392 -3.56429277  0.92083926  1.58695602]\n",
      " [-0.1907453   3.88654445 -0.9924596   0.50894334 -1.54094797]\n",
      " [ 1.36975257 -0.949706   -1.21502842 -1.01219414  1.17766926]\n",
      " [ 6.68364639 -3.63290504  5.84492569  3.05703556 -5.66934565]\n",
      " [ 1.11094749  0.09952239  3.86083461  1.45311888 -2.11112746]\n",
      " [ 0.10986398  2.13384065 -2.62225335 -2.80250807  6.23969078]\n",
      " [ 3.29361941  7.8038488  -4.13355028 -0.04008302  4.15263921]\n",
      " [-5.08482112 -1.6771261  -1.293041   -0.7298814   2.76906524]\n",
      " [-2.30295519 -2.65330525  0.65602384  3.00501587 -5.8811091 ]\n",
      " [-0.21924211 -0.44924887  3.87631796  0.45510407 -0.97592215]\n",
      " [-0.22088038 -4.02412324  2.3911621   2.55048646 -4.25144675]\n",
      " [ 2.80865208  3.65267906 -1.65379397  1.86969535  1.39514989]\n",
      " [ 0.79294642 -0.97704811  2.31465216  2.0320489  -0.92616106]\n",
      " [-3.01171075 -2.30548849 -1.13200347  1.13987352 -1.97707209]\n",
      " [ 6.21434024  1.75413427  1.04787598  0.14412573 -1.37908413]\n",
      " [ 6.20038062 -5.50555605  6.00884933  1.3502165  -4.70107605]\n",
      " [ 4.49306869  5.03263259 -2.59978875 -2.97492992  5.8179039 ]\n",
      " [ 1.70613897  6.16014672 -1.68403347  2.59070332  3.51902406]\n",
      " [ 3.44900464  6.32213485  5.27968237  3.07530425 -6.90325902]\n",
      " [-2.12370624  1.79931158  1.17538677 -0.09736442  0.19900814]]\n",
      "[[ 0.13861739  0.45240161 -3.75474031  1.77751768  1.48995401]\n",
      " [-0.37805174  3.41943213 -1.18290714  1.36562176 -1.63794998]\n",
      " [ 1.18244613 -1.41681831 -1.40547596 -0.15551571  1.08066725]\n",
      " [ 6.49633995 -4.10001735  5.65447815  3.91371398 -5.76634766]\n",
      " [ 0.92364105 -0.36758992  3.67038707  2.3097973  -2.20812947]\n",
      " [-0.07744246  1.66672834 -2.81270089 -1.94582964  6.14268877]\n",
      " [ 3.10631297  7.33673649 -4.32399782  0.81659541  4.0556372 ]\n",
      " [-5.27212756 -2.14423841 -1.48348854  0.12679703  2.67206323]\n",
      " [-2.49026163 -3.12041756  0.4655763   3.8616943  -5.97811111]\n",
      " [-0.40654855 -0.91636118  3.68587043  1.31178249 -1.07292416]\n",
      " [-0.40818682 -4.49123555  2.20071456  3.40716488 -4.34844876]\n",
      " [ 2.62134564  3.18556675 -1.84424151  2.72637378  1.29814788]\n",
      " [ 0.60563998 -1.44416043  2.12420462  2.88872733 -1.02316307]\n",
      " [-3.19901719 -2.7726008  -1.32245101  1.99655194 -2.0740741 ]\n",
      " [ 6.0270338   1.28702195  0.85742844  1.00080416 -1.47608614]\n",
      " [ 6.01307418 -5.97266836  5.81840179  2.20689493 -4.79807806]\n",
      " [ 4.30576225  4.56552028 -2.79023629 -2.11825149  5.72090189]\n",
      " [ 1.51883253  5.69303441 -1.87448101  3.44738175  3.42202205]\n",
      " [ 3.2616982   5.85502254  5.08923483  3.93198267 -7.00026103]\n",
      " [-2.31101268  1.33219927  0.98493923  0.75931401  0.10200613]]\n"
     ]
    }
   ],
   "source": [
    "print \"=== For autograder ===\"\n",
    "forward_backward_prop(data, labels, params)\n",
    "#gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
