{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will walk you through the process of implementing \n",
    "\n",
    "- A softmax function\n",
    "- A simple neural network\n",
    "- Back propagation\n",
    "- Word2vec models\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD) for a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading help data...\n"
     ]
    }
   ],
   "source": [
    "using PyPlot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax\n",
    "\n",
    ">If you want the outputs of a network to be interpretable as posterior\n",
    ">probabilities for a categorical target variable, it is highly desirable for\n",
    ">those outputs to lie between zero and one and to sum to one. The purpose of\n",
    ">the softmax activation function is to enforce these constraints on the\n",
    ">outputs. \n",
    "\n",
    "http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html\n",
    "\n",
    "\n",
    "$$softmax(x) = softmax(x + c)$$\n",
    "where $x + c$ means adding the constant $c$ to every dimension of $x$.\n",
    "\n",
    "Note: In practice, we make use of this property and choose $c = âˆ’ max_ix_i$ when computing softmax probabil-\n",
    "ities for numerical stability (i.e. subtracting its maximum element from all elements of x).\n",
    "\n",
    ">Hence you can always pick one of the output units, and\n",
    "add an appropriate constant to each net input to produce any desired net\n",
    "input for the selected output unit, which you can choose to be zero or\n",
    "whatever is convenient. You can use the same trick to make sure that none of\n",
    "the exponentials overflows.\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x)\n",
    "    # Softmax function #\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.\n",
    "    #\n",
    "    # http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression\n",
    "    ###################################################################\n",
    "    ### YOUR CODE HERE\n",
    "    # find max element per row\n",
    "    row = size(x,1);\n",
    "    xMax = zeros(size(x));\n",
    "    for r = 1:row\n",
    "        xMax[r,:] = exp(x[r,:] - maximum(x[r,:]))/sum(exp(x[r,:]-maximum(x[r,:]))) ;\n",
    "    end\n",
    "    x = xMax;\n",
    "    ### END YOUR CODE\n",
    "    return x;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[0.2689414213699951 0.7310585786300049\n",
      " 0.2689414213699951 0.7310585786300049]\n",
      "[0.2689414213699951 0.7310585786300049\n",
      " 0.2689414213699951 0.7310585786300049]\n",
      "[0.7310585786300049 0.2689414213699951]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "println(\"=== For autograder ===\");\n",
    "println(softmax([[1 2],[3 4]]));\n",
    "println(softmax([[1001 1002],[3 4]]));\n",
    "println(softmax([[-1001 -1002]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "In this part, we're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(x)\n",
    "    # Sigmoid function #\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1.0./(1.0+exp(-x));\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid_grad (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid_grad(f)\n",
    "    # Sigmoid gradient function #\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = f.*(1.0-f);\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[0.7310585786300049 0.8807970779778823\n",
      " 0.2689414213699951 0.11920292202211755]\n",
      "[0.19661193324148185 0.10499358540350662\n",
      " 0.19661193324148185 0.1049935854035065]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = [[1 2], [-1 -2]];\n",
    "f = sigmoid(x);\n",
    "g = sigmoid_grad(f);\n",
    "println(\"=== For autograder ===\");\n",
    "println(f);\n",
    "println(g);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the functions implemented above to implement a neural network with one sigmoid hidden layer. \n",
    "\n",
    "To calculate the numerical gradient we'll use:\n",
    "$$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x - h)}{2h} \\hspace{0.1in}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradcheck_naive (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "function gradcheck_naive(f, x)\n",
    "    ###\n",
    "    # Gradient check for a function f \n",
    "    # - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    # - x is the point (numpy array) to check the gradient at\n",
    "    ### \n",
    "\n",
    "    #rndstate = random.getstate()\n",
    "    #random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    gradsz = size(grad)\n",
    "    # TODO: use eachindex(A) from julia 0.4 >>\n",
    "    if(length(gradsz) == 1)\n",
    "        gradidx = {i for i=1:gradsz[1]}\n",
    "    elseif (length(gradsz) == 2)\n",
    "        gradidx = {[i,j] for j=1:gradsz[2], i=1:gradsz[1]}\n",
    "    end\n",
    "    gradidx = reshape(gradidx,length(gradidx),1);\n",
    "    # << TODO: use eachindex(A) from julia 0.4\n",
    "    for ix in gradidx\n",
    "        println(x[ix])\n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        fxBefore = f(x[ix]-h)[1];\n",
    "        fxAfter = f(x[ix]+h)[1];\n",
    "        numgrad = (fxAfter - fxBefore)/(2*h);\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if(reldiff > 1e-5)\n",
    "            println(\"Gradient check failed.\")\n",
    "            println(\"First gradient error found at index \",ix)\n",
    "            println(\"Your gradient: \", grad[ix] ,\" \\t Numerical gradient:\", numgrad)\n",
    "            return\n",
    "        end\n",
    "    \n",
    "    end\n",
    "        println(\"Gradient check passed!\");\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quad (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function quad(x)\n",
    "    return sum(x.^2),[x*2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "123.456\n",
      "Gradient check passed!\n",
      "-1.6717534192035735\n",
      "0.2269707364326208\n",
      "-0.34250309174718774\n",
      "Gradient check passed!\n",
      "[-0.559166363294027,-0.559166363294027]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "`isless` has no method matching isless(::Float64, ::Array{Float64,2})\nwhile loading In[39], in expression starting on line 7",
     "output_type": "error",
     "traceback": [
      "`isless` has no method matching isless(::Float64, ::Array{Float64,2})\nwhile loading In[39], in expression starting on line 7",
      "",
      " in < at operators.jl:32",
      " in gradcheck_naive at In[37]:37"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "#quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "println(\"=== For autograder ===\")\n",
    "#gradcheck_naive(quad, 123.456)      # scalar test\n",
    "#gradcheck_naive(quad, randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
