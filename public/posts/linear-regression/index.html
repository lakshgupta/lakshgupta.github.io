<!doctype html><html lang=en-us><head><title>Linear Regression | Laksh Gupta</title><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Using a single neuron to perform linear regression and train via gradient descent."><meta name=generator content="Hugo 0.155.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon></head><body><nav class=site-nav><style>.site-nav{display:flex;justify-content:flex-end;align-items:center;gap:.6rem}.site-nav a{text-decoration:none;color:inherit}.site-nav a{margin-left:.6rem}</style><div class=nav-right><a href=/>Home</a>
<a href=/posts/>Archive</a>
<a href=/tags/>Tags</a>
<a href=/about/>About</a>
<a href=/index.xml>RSS</a></div></nav><main class=main><section id=single><h1 class=title>Linear Regression</h1><div class=tip><time datetime="2015-05-27 12:00:00 +0000 UTC">May 27, 2015</time>
<span class=split>·
</span><span>552 words
</span><span class=split>·
</span><span>3 minute read</span></div><div class=content><p>Alright, in the last post we looked at the very basic building block of a neural network: a neuron. But what could possibly a single neuron be good for? Well, as I mentioned in my last post it can be used to learn very simple models. Let us try to solve a linear regression problem using a neuron.</p><blockquote>Linear regression is the simplest form of regression. We model our system with a linear combination of features to produce one output.<p align=right>- <a href=http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1>Brian Dolhansky</a></p></blockquote><h2 class=section-heading>The Problem</h2><p>I&rsquo;ll use the problem used in the Andrew Ng&rsquo;s machine learning course. The dataset is located here: <code>/data/ex1data1.txt</code>. We will try to predict the profit for the franchise based on the population of the city. We&rsquo;ll use the previous data to prepare a model. So let us first understand the data.</p><center><canvas id=inputData width=600 height=400></canvas></center><p>Looking at the data we can say that we don&rsquo;t need a complex model and linear regression is good enough for our purpose.</p><h2 class=section-heading>Training a model</h2><center><canvas id=artificialneuron width=500 height=150></canvas></center><p>Our neuron will receive two values as an input. One of them is the actual value from the data and the other is a bias value. We usually include the bias value along with the input feature matrix x.</p><blockquote>b is the bias, a term that shifts the decision boundary away from the origin and does not depend on any input value.<p align=right>- <a href=http://en.wikipedia.org/wiki/Perceptron>Wikipedia</a></p></blockquote><p>Since we want to linearly fit the data, we&rsquo;ll use the linear activation function. When our neuron will receive the inputs, we&rsquo;ll calculate the weighted sum and consider that as our output from the neuron.</p><center>$$f(x_i,w) = \phi(\sum\limits_{j=0}^n(w^j x_i^j)) = \sum\limits_{j=0}^n(w^j x_i^j) = w^Tx_i$$</center>where<ul><li>$$i$$ represents a row of a matrix</li><li>$$j$$ represetns an element of a matrix</li></ul><p>The other way to look at our setup is that we are trying to fit a line to the data represented as</p><p>$$y_i = w^0x_i^0 + w^1b$$</p><p>We then try to figure out how close our neuron output or prediction is from the actual answer, i.e. we&rsquo;ll apply a loss function, also known as a cost function over our dataset. A commonly used one is the least square error:</p><center>$$J(w) = \sum\limits_{i=0}^n(f(x_i,w) - y_i)^2$$</center>The idea is to use this value to modify our randomly initialized weight matrix till the time we stop observing the decrease in the cost function value. The method we'll use to modify the weight matrix is known as Gradient Descent.<center>$$w = w - \frac{\alpha}{m}\Delta J(w)$$</center>here<ul><li>$$w$$ is the weight matrix</li><li>$$\alpha$$ is the learning rate</li><li>$$m$$ is the size of our data acting as a normalizing factor</li><li>$$\Delta J(w)$$ is the gradient of the cost function with respect to each of the weight under consideration say weight for the connection between a neuron $$j$$ and a neuron $$k$$</li></ul><p>$$\frac{\partial}{\partial w_{jk}} J(w) = \sum\limits_{i=0}^n 2\left(f(x_i, w)-y_i\right) \frac{\partial}{\partial w_{jk}} f(x_i, w) $$</p><p>So let us train the model and see how it is behaving by plotting the results of the above equation in red using the weight matrix and the x-axis.</p><center><canvas id=fitData height=400px width=600>
This text is displayed if your browser does not support HTML5 Canvas.</canvas></center><h2 class=section-heading>Making a Prediction</h2>To make a prediction we just need to use the modified weight matrix, obtained after the gradient descent step, along with the new input values and apply the same function we used above:<center>$$f(x_i,w) = w^Tx_i$$</center><script language=javascript type=text/javascript src=http://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.js></script><script language=javascript type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.4.4/p5.min.js></script><script language=javascript type=text/javascript src=/js/plot/scatter.js></script><script language=javascript type=text/javascript src=/js/utils/mathUtils.js></script><script language=javascript type=text/javascript src=/js/nn/canvas.js></script><script language=javascript type=text/javascript src=/js/nn/neuron.js></script><script language=javascript type=text/javascript src=/js/nn/neuralnet.js></script><script language=javascript>var neuronOut,iterations,learningRate,_ancanvas=document.getElementById("artificialneuron"),_anctx=_ancanvas.getContext("2d"),neuronIn1=new neuron(_anctx,50,40,neuronRadius,"b"),neuronIn2=new neuron(_anctx,50,110,neuronRadius,"x_i^j"),hiddenLayer=new neuron(_anctx,200,75,neuronRadius);_anctx.mathText("f(x_i, w)",200,120,{"text-align":"center"}),neuronOut=new neuron(_anctx,350,75,neuronRadius,"y"),connectLayers([neuronIn1,neuronIn2],[hiddenLayer]),connectLayers([hiddenLayer],[neuronOut]),iterations=1e3,learningRate=.01;function setup(){loadTable("/data/ex1data1.txt","CSV",linReg)}function linReg(e){var t,s,i,a,r,c,l,d,u,m,f,p,g,v,b,j,h=e.rows.length-1,n=Array.matrix(h,2,0),o=Array.matrix(h,1,0);J_history=Array.matrix(iterations,1,0);for(m=n.length,s=Array.matrix(2,1,0),d=e.getNum(0,0),l=e.getNum(0,0),c=e.getNum(0,1),r=e.getNum(0,1),t=0;t<h;t++)n[t][0]=e.getNum(t,0),n[t][1]=1,o[t][0]=e.getNum(t,1),d<n[t][0]&&(d=n[t][0]),l>n[t][0]&&(l=n[t][0]),c<o[t][0]&&(c=o[t][0]),r>o[t][0]&&(r=o[t][0]);f={y:{min:r,max:c,steps:5,label:"Profit in $10,000s"},x:{min:l,max:d,steps:5,label:"Population of City in 10,000s"}},j=new scatter("inputData",f,n,o),console.log("Initial cost: "+computeCost(n,o,s)),console.log("Initial theta: "+s);for(t=0;t<iterations;t++){for(p=s,g=numeric.sub(numeric.dot(n,p),o),a=0;a<s.length;a++){for(u=g.slice(0),i=0;i<m;i++)u[i]=u[i]*n[i][a];correction=learningRate/m*numeric.sum(u),u=new Array,s[a]=s[a]-correction}J_history[t]=computeCost(n,y,s)}console.log("Cost history: "+J_history),console.log("Final theta: "+s),v={y:{min:r,max:c,steps:5,label:"Profit in $10,000s"},x:{min:l,max:d,steps:5,label:"Population of City in 10,000s"}},b=new scatter("fitData",v,n,o),b.plotLine(s)}function computeCost(e,t,n){var s=1;return Array.isArray(e)&&(s=e.length),numeric.sum(numeric.pow(numeric.sub(numeric.dot(e,n),t),2))}</script></div><div class=tags><a href=https://lakshgupta.github.io/tags/machine-learning>machine-learning</a>
<a href=https://lakshgupta.github.io/tags/linear-regression>linear-regression</a></div></section></main><footer style=text-align:center><p>&copy; 2026 Laksh Gupta</p></footer></body></html>